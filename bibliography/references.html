<h1 id="references">References</h1>

<h1 id="the-order-of-reference-should-be-the-same-as-they-appear-on-the-page">the order of reference should be the same as they appear on the page</h1>

<p>Agarwal, A., Bird, S., Cozowicz, M., Hoang, L., Langford, J., Lee, S., … Slivkins, A. (2016). A Multiworld Testing Decision Service. <i>CoRR</i>, <i>abs/1606.03966</i>. Retrieved from http://arxiv.org/abs/1606.03966</p>

<p>Li, L., Chu, W., Langford, J., &amp; Schapire, R. E. (2010). A Contextual-Bandit Approach to Personalized News Article Recommendation. <i>CoRR</i>, <i>abs/1003.0146</i>. Retrieved from http://arxiv.org/abs/1003.0146</p>

<p>Horvitz, D. G., &amp; Thompson, D. J. (1952). A Generalization of Sampling Without Replacement from a Finite Universe. <i>Journal of the American Statistical Association</i>, <i>47</i>(260), 663–685. https://doi.org/10.1080/01621459.1952.10483446</p>

<p>Jiang, N., &amp; Li, L. (2016). Doubly Robust Off-policy Value Evaluation for Reinforcement Learning. In <i>Proceedings of the 33nd International Conference on Machine Learning,
 ICML 2016, New York City, NY, USA, June 19-24, 2016</i> (pp. 652–661). Retrieved from http://proceedings.mlr.press/v48/jiang16.html</p>

<p>Dudı́k Miroslav, Langford, J., &amp; Li, L. (2011). Doubly Robust Policy Evaluation and Learning. In <i>Proceedings of the 28th International Conference on Machine Learning,
 ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011</i> (pp. 1097–1104). Retrieved from https://icml.cc/2011/papers/554_icmlpaper.pdf</p>

<p>Bietti, A., Agarwal, A., &amp; Langford, J. (2018). A Contextual Bandit Bake-off. arXiv:1802.04064v3 [stat.ML]. Retrieved from https://www.microsoft.com/en-us/research/publication/a-contextual-bandit-bake-off-2/</p>

<p>Karampatziakis, N., &amp; Langford, J. (2011). Online Importance Weight Aware Updates. In <i>Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</i> (pp. 392–399). Arlington, Virginia, United States: AUAI Press. Retrieved from http://dl.acm.org/citation.cfm?id=3020548.3020594</p>

<p>Osband, I., &amp; Roy, B. V. (2015). Bootstrapped Thompson Sampling and Deep Exploration. <i>CoRR</i>, <i>abs/1507.00300</i>. Retrieved from http://arxiv.org/abs/1507.00300</p>

<p>Eckles, D., &amp; Kaptein, M. (2014). Thompson sampling with the online bootstrap. <i>CoRR</i>, <i>abs/1410.4009</i>. Retrieved from http://arxiv.org/abs/1410.4009</p>

<p>Agarwal, A., Hsu, D. J., Kale, S., Langford, J., Li, L., &amp; Schapire, R. E. (2014). Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits. <i>CoRR</i>, <i>abs/1402.0555</i>. Retrieved from http://arxiv.org/abs/1402.0555</p>

<p>Cortes, D. (2018). Adapting multi-armed bandits policies to contextual bandits scenarios. <i>CoRR</i>, <i>abs/1811.04383</i>. Retrieved from http://arxiv.org/abs/1811.04383</p>
