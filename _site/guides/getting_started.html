<!doctype html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" type="image/x-icon" href="/vowpalwabbit.github.io/assets/images/favicon.png">
    <link rel="stylesheet" href="/vowpalwabbit.github.io/assets/bootstrap-4.1.3-dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="/vowpalwabbit.github.io/assets/syntax.css">
    <link rel="stylesheet" href="/vowpalwabbit.github.io/assets/main.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Work+Sans&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="/vowpalwabbit.github.io/assets/nav.js"></script>
    <title>Get started with Vowpal Wabbit</title>
  </head>
  <body class=tutorial>
    

<div class="container-fluid nav_bar_container">
  <div class="container navbar navbar-expand-lg">
    <div class="logo">
      <a href="/vowpalwabbit.github.io">
        <?xml version="1.0" encoding="UTF-8"?>
<svg width="260px" height="45px" viewBox="0 0 260 45" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 53 (72520) - https://sketchapp.com -->
    <title>VW_Logo_Lock_up</title>
    <desc>Created with Sketch.</desc>
    <defs>
        <polygon id="path-1" points="0 0 42 0 42 45 0 45"></polygon>
    </defs>
    <g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="HD_Desktop_header_Home" transform="translate(-258.000000, -44.000000)">
            <g id="Header">
                <g id="VW_Logo" transform="translate(258.000000, 28.000000)">
                    <g transform="translate(27.000000, 0.000000)" fill="#FFFFFF" id="Logo">
                        <g transform="translate(30.000000, 31.000000)">
                            <path d="M10.632,22 L5.784,22 L0.144,6.16 L4.488,6.16 L7.392,15.736 L8.256,18.856 L9.096,15.784 L12.048,6.16 L16.248,6.16 L10.632,22 Z M24.816,5.92 C26.416008,5.92 27.7959942,6.24399676 28.956,6.892 C30.1160058,7.54000324 31.0079969,8.47599388 31.632,9.7 C32.2560031,10.9240061 32.568,12.3839915 32.568,14.08 C32.568,15.7760085 32.2560031,17.2359939 31.632,18.46 C31.0079969,19.6840061 30.1160058,20.6199968 28.956,21.268 C27.7959942,21.9160032 26.416008,22.24 24.816,22.24 C23.215992,22.24 21.8320058,21.9160032 20.664,21.268 C19.4959942,20.6199968 18.6000031,19.6840061 17.976,18.46 C17.3519969,17.2359939 17.04,15.7760085 17.04,14.08 C17.04,12.3839915 17.3519969,10.9240061 17.976,9.7 C18.6000031,8.47599388 19.4959942,7.54000324 20.664,6.892 C21.8320058,6.24399676 23.215992,5.92 24.816,5.92 Z M24.816,9.16 C23.6639942,9.16 22.784003,9.57599584 22.176,10.408 C21.567997,11.2400042 21.264,12.4639919 21.264,14.08 C21.264,15.6960081 21.567997,16.9199958 22.176,17.752 C22.784003,18.5840042 23.6639942,19 24.816,19 C25.9680058,19 26.843997,18.5840042 27.444,17.752 C28.044003,16.9199958 28.344,15.6960081 28.344,14.08 C28.344,12.4639919 28.044003,11.2400042 27.444,10.408 C26.843997,9.57599584 25.9680058,9.16 24.816,9.16 Z M42.72,22 L38.088,22 L33.528,6.16 L37.968,6.16 L40.584,18.76 L43.44,6.16 L47.184,6.16 L50.088,18.736 L52.704,6.16 L56.976,6.16 L52.392,22 L47.904,22 L46.032,14.32 L45.312,10.456 L45.264,10.456 L44.568,14.32 L42.72,22 Z M66.24,6.16 C68.2080098,6.16 69.7199947,6.61199548 70.776,7.516 C71.8320053,8.42000452 72.36,9.71999152 72.36,11.416 C72.36,13.0320081 71.8320053,14.2839956 70.776,15.172 C69.7199947,16.0600044 68.2080098,16.504 66.24,16.504 L63.096,16.504 L63.096,22 L58.968,22 L58.968,6.16 L66.24,6.16 Z M65.592,13.48 C66.4880045,13.48 67.1479979,13.3080017 67.572,12.964 C67.9960021,12.6199983 68.208,12.0880036 68.208,11.368 C68.208,10.6639965 67.9960021,10.1400017 67.572,9.796 C67.1479979,9.45199828 66.4880045,9.28 65.592,9.28 L63.096,9.28 L63.096,13.48 L65.592,13.48 Z M82.464,18.424 L76.992,18.424 L75.888,22 L71.736,22 L77.304,6.16 L82.248,6.16 L87.792,22 L83.544,22 L82.464,18.424 Z M81.6,15.616 L80.448,11.824 L79.752,9.328 L79.704,9.328 L79.056,11.8 L77.856,15.616 L81.6,15.616 Z M101.712,18.808 L101.712,22 L89.64,22 L89.64,6.16 L93.768,6.16 L93.768,18.808 L101.712,18.808 Z M117.648,22 L113.016,22 L108.456,6.16 L112.896,6.16 L115.512,18.76 L118.368,6.16 L122.112,6.16 L125.016,18.736 L127.632,6.16 L131.904,6.16 L127.32,22 L122.832,22 L120.96,14.32 L120.24,10.456 L120.192,10.456 L119.496,14.32 L117.648,22 Z M141.624,18.424 L136.152,18.424 L135.048,22 L130.896,22 L136.464,6.16 L141.408,6.16 L146.952,22 L142.704,22 L141.624,18.424 Z M140.76,15.616 L139.608,11.824 L138.912,9.328 L138.864,9.328 L138.216,11.8 L137.016,15.616 L140.76,15.616 Z M148.8,6.16 L156.504,6.16 C160.312019,6.17600008 162.216,7.59998584 162.216,10.432 C162.216,11.3600046 161.944003,12.1439968 161.4,12.784 C160.855997,13.4240032 160.056005,13.8239992 159,13.984 L159,14.008 C160.184006,14.1680008 161.083997,14.5799967 161.7,15.244 C162.316003,15.9080033 162.624,16.735995 162.624,17.728 C162.624,19.1040069 162.136005,20.1599963 161.16,20.896 C160.183995,21.6320037 158.752009,22 156.864,22 L148.8,22 L148.8,6.16 Z M155.76,12.64 C157.248007,12.64 157.992,12.0640058 157.992,10.912 C157.992,9.75999424 157.248007,9.184 155.76,9.184 L152.784,9.184 L152.784,12.64 L155.76,12.64 Z M156.168,19.096 C156.904004,19.096 157.459998,18.9440015 157.836,18.64 C158.212002,18.3359985 158.4,17.8960029 158.4,17.32 C158.4,16.7599972 158.212002,16.3320015 157.836,16.036 C157.459998,15.7399985 156.904004,15.592 156.168,15.592 L152.784,15.592 L152.784,19.096 L156.168,19.096 Z M165.216,6.16 L172.92,6.16 C176.728019,6.17600008 178.632,7.59998584 178.632,10.432 C178.632,11.3600046 178.360003,12.1439968 177.816,12.784 C177.271997,13.4240032 176.472005,13.8239992 175.416,13.984 L175.416,14.008 C176.600006,14.1680008 177.499997,14.5799967 178.116,15.244 C178.732003,15.9080033 179.04,16.735995 179.04,17.728 C179.04,19.1040069 178.552005,20.1599963 177.576,20.896 C176.599995,21.6320037 175.168009,22 173.28,22 L165.216,22 L165.216,6.16 Z M172.176,12.64 C173.664007,12.64 174.408,12.0640058 174.408,10.912 C174.408,9.75999424 173.664007,9.184 172.176,9.184 L169.2,9.184 L169.2,12.64 L172.176,12.64 Z M172.584,19.096 C173.320004,19.096 173.875998,18.9440015 174.252,18.64 C174.628002,18.3359985 174.816,17.8960029 174.816,17.32 C174.816,16.7599972 174.628002,16.3320015 174.252,16.036 C173.875998,15.7399985 173.320004,15.592 172.584,15.592 L169.2,15.592 L169.2,19.096 L172.584,19.096 Z M185.76,22 L181.632,22 L181.632,6.16 L185.76,6.16 L185.76,22 Z M202.584,9.352 L197.424,9.352 L197.424,22 L193.296,22 L193.296,9.352 L188.112,9.352 L188.112,6.16 L202.584,6.16 L202.584,9.352 Z" id="VOWPALWABBIT" fill-rule="nonzero"></path>
                        </g>
                    </g>
                    <g id="Group-3" transform="translate(0.000000, 16.000000)">
                        <mask id="mask-2" fill="white">
                            <use xlink:href="#path-1"></use>
                        </mask>
                        <g id="Clip-2"></g>
                        <path d="M41.9813276,24.9175791 C41.9813276,13.8292163 32.9500091,14.0405396 32.9500091,14.0405396 C32.0605466,9.2330546 30.5043762,5.39449409 29.0122432,2.95421329 C27.2686285,0.654814673 25.9862033,0 25.9862033,0 C24.433145,1.05110573 23.6649347,5.90914536 23.6649347,5.90914536 C21.2590408,1.77276757 18.4947765,1.57941395 18.4947765,1.57941395 C14.9073614,12.5619954 25.5641545,16.6805233 25.5641545,16.6805233 C14.9631398,16.6805233 9.49960631,22.5600786 6.88927134,26.9815751 C6.00639209,28.5787191 5.40479894,30.0454034 5.02572104,31.1057335 L4.49702287,32.7908091 C2.84928484,31.8162542 0.017033097,31.7817524 0.017033097,31.7817524 C-0.418062629,43.820471 7.62683939,42.0251815 7.62683939,42.0251815 C7.62683939,43.7148094 9.84265565,45 9.84265565,45 L27.1467778,45 C27.1467778,45 26.8872764,42.8218367 25.0774697,41.6425474 C24.7795458,41.2040875 24.6799585,40.8190574 25.0346184,40.4133023 C26.0320469,39.2724681 26.7444788,40.7698205 26.7444788,40.7698205 L27.4953331,41.7830702 C29.6309529,44.4419819 31.1160238,44.9275223 31.7469425,45 L32.0875979,45 C30.8645421,41.2514076 30.9082312,37.9785322 30.9082312,37.9785322 C30.7234202,33.2048302 34.2161557,28.8249036 34.2161557,28.8249036 C42.7627045,28.8249036 41.9813276,24.9175791 41.9813276,24.9175791" id="Fill-1" fill="#FEFEFE" mask="url(#mask-2)"></path>
                    </g>
                </g>
            </g>
        </g>
    </g>
</svg>
      </a>
    </div>

    <div class="nav">
      <a
        href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki"
        target="_blank"
        class="nav_item"
      >
        <?xml version="1.0" encoding="UTF-8"?>
<svg width="22px" height="30px" viewBox="0 0 22 30" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 53 (72520) - https://sketchapp.com -->
    <title>noun_Document_1743070</title>
    <desc>Created with Sketch.</desc>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Artboard" fill="#FFFFFF" fill-rule="nonzero">
            <g id="noun_Document_1743070">
                <g id="Group">
                    <path d="M1.03125,0.0158587653 C0.451034375,0.0199131328 0.003540625,0.591827667 0,1.03796823 L0,28.9756438 C6.875e-05,29.5107863 0.491321875,29.9976852 1.03125,29.9977533 L20.96875,29.9977533 C21.5086781,29.9976852 21.9999312,29.5107863 22,28.9756438 L22,7.17062498 C22,6.90279483 21.8899656,6.63635134 21.6992187,6.44663078 L15.5117187,0.313974024 C15.3202844,0.124924659 15.0514719,0.0152148363 14.78125,0.0158587653 L1.03125,0.0158587653 Z M2.0625,2.06007769 L13.75,2.06007769 L13.75,7.17062498 C13.7500687,7.70578787 14.2413219,8.19268334 14.78125,8.19273444 L19.9375,8.19273444 L19.9375,27.9535344 L2.0625,27.9535344 L2.0625,2.06007769 Z M15.8125,3.49741911 L18.4872875,6.14851552 L15.8125,6.14851552 L15.8125,3.49741911 Z M4.8125,5.80781237 C4.242975,5.80781237 3.78125,6.26541078 3.78125,6.82992183 C3.78125,7.39439882 4.242975,7.85203129 4.8125,7.85203129 L9.625,7.85203129 C10.1945594,7.85203129 10.65625,7.39439882 10.65625,6.82992183 C10.65625,6.26541078 10.1945594,5.80781237 9.625,5.80781237 L4.8125,5.80781237 Z M4.8125,11.2590628 C4.242975,11.2590628 3.78125,11.7166612 3.78125,12.2811723 C3.78125,12.8456493 4.242975,13.3032817 4.8125,13.3032817 L17.1875,13.3032817 C17.7570594,13.3032817 18.21875,12.8456493 18.21875,12.2811723 C18.21875,11.7166612 17.7570594,11.2590628 17.1875,11.2590628 L4.8125,11.2590628 Z M4.8125,16.7103303 C4.242975,16.7103303 3.78125,17.1679287 3.78125,17.7324398 C3.78125,18.2969168 4.242975,18.7545492 4.8125,18.7545492 L17.1875,18.7545492 C17.7570594,18.7545492 18.21875,18.2969168 18.21875,17.7324398 C18.21875,17.1679287 17.7570594,16.7103303 17.1875,16.7103303 L4.8125,16.7103303 Z M4.8125,22.1615808 C4.242975,22.1615808 3.78125,22.6191792 3.78125,23.1836902 C3.78125,23.7481672 4.242975,24.2057997 4.8125,24.2057997 L17.1875,24.2057997 C17.7570594,24.2057997 18.21875,23.7481672 18.21875,23.1836902 C18.21875,22.6191792 17.7570594,22.1615808 17.1875,22.1615808 L4.8125,22.1615808 Z" id="Shape"></path>
                </g>
            </g>
        </g>
    </g>
</svg>

        <span class="nav_item_text">Documentation</span>
      </a>

      <a
        href="https://github.com/VowpalWabbit/vowpal_wabbit"
        target="_blank"
        class="nav_item"
      >
        <?xml version="1.0" encoding="UTF-8"?>
<svg width="30px" height="30px" viewBox="0 0 30 30" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 53 (72520) - https://sketchapp.com -->
    <title>GitHub</title>
    <desc>Created with Sketch.</desc>
    <g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="HD_Desktop_header_Home" transform="translate(-1538.000000, -58.000000)" fill="#FFFFFF">
            <g id="Header">
                <path d="M1568,73.7951128 C1567.94603,74.2583723 1567.89891,74.7224863 1567.83682,75.1847206 C1567.56271,77.2238487 1566.86543,79.1138828 1565.7732,80.8526868 C1564.55268,82.7956941 1562.9969,84.4188114 1561.05088,85.6538569 C1560.03339,86.2996179 1558.94817,86.8066231 1557.80753,87.1895684 C1557.17559,87.4016322 1556.73848,87.0743943 1556.73531,86.4093237 C1556.72907,85.1020808 1556.73745,83.7947524 1556.73164,82.487424 C1556.72804,81.6908602 1556.64825,80.9063436 1556.21259,80.2087201 C1556.0855,80.0052003 1555.92463,79.8226135 1555.77393,79.6234512 C1556.35935,79.5140871 1556.9463,79.4362506 1557.51632,79.2916849 C1559.04696,78.9034423 1560.41543,78.2275208 1561.34791,76.8924242 C1562.03287,75.9115647 1562.34632,74.7851144 1562.48436,73.6108172 C1562.59169,72.6982249 1562.62222,71.7826422 1562.42971,70.8740656 C1562.23608,69.9599354 1561.84044,69.1418405 1561.22834,68.4359293 C1561.07756,68.2620575 1561.04763,68.1215929 1561.12221,67.896713 C1561.52468,66.6818314 1561.3562,65.491984 1560.9042,64.3262309 C1560.87829,64.2595017 1560.76574,64.1973008 1560.68569,64.1853391 C1560.15364,64.1058793 1559.65196,64.2559986 1559.18533,64.4761793 C1558.42904,64.8332361 1557.69764,65.242668 1556.9487,65.6159585 C1556.82229,65.6790137 1556.64782,65.7219904 1556.51706,65.6889249 C1554.16941,65.0964789 1551.82595,65.097077 1549.47805,65.6883268 C1549.34078,65.7228448 1549.14638,65.677134 1549.02331,65.5987849 C1548.07109,64.9919849 1547.06446,64.5120644 1545.97001,64.2236166 C1545.9511,64.218661 1545.93272,64.2115694 1545.91364,64.2077246 C1545.22551,64.0670892 1545.09971,64.1456947 1544.9036,64.8108506 C1544.58682,65.884755 1544.5296,66.9550708 1544.90668,68.0296587 C1544.93627,68.1139033 1544.90129,68.2616303 1544.8404,68.3306664 C1543.60397,69.7341156 1543.31096,71.3989279 1543.46559,73.1936101 C1543.55933,74.2817831 1543.81162,75.3262104 1544.29869,76.3083514 C1544.97176,77.6654063 1546.11181,78.4952066 1547.4926,78.982475 C1548.35956,79.2884382 1549.27852,79.4473579 1550.18824,79.6758264 C1550.19526,79.6319098 1550.20056,79.6666842 1550.18568,79.6820635 C1549.71726,80.1697591 1549.45556,80.755028 1549.34531,81.4170227 C1549.33137,81.5007546 1549.27365,81.6203716 1549.20685,81.6454912 C1548.16482,82.0372369 1547.11706,82.2433199 1546.08683,81.6149888 C1545.61867,81.3295314 1545.27683,80.9169382 1545.00101,80.4479541 C1544.56852,79.712395 1543.95864,79.179074 1543.13923,78.9361661 C1542.84434,78.8487602 1542.50823,78.8709748 1542.19359,78.8892591 C1541.97909,78.9018189 1541.90862,79.062362 1542.05256,79.2422147 C1542.18529,79.4077988 1542.32025,79.5950849 1542.4978,79.6971865 C1543.28625,80.1507913 1543.72798,80.8753286 1544.08881,81.6641173 C1544.30382,82.1341266 1544.48812,82.6140471 1544.88675,82.9753759 C1545.52169,83.5510755 1546.29115,83.7682658 1547.11612,83.8044072 C1547.81434,83.834995 1548.51496,83.8107298 1549.24722,83.8107298 C1549.24611,83.8009041 1549.26073,83.8742123 1549.26167,83.9476058 C1549.27211,84.7377616 1549.2828,85.5279173 1549.28844,86.3180731 C1549.294,87.0913115 1548.82935,87.4109452 1548.0952,87.1563319 C1544.28705,85.8355039 1541.4928,83.3342269 1539.62819,79.7992882 C1538.22405,77.1372118 1537.78027,74.2780237 1538.09739,71.2935795 C1538.4294,68.1688416 1539.66702,65.4337134 1541.73193,63.0872551 C1543.89022,60.6347648 1546.57714,59.0045559 1549.78825,58.3523869 C1554.45335,57.4048494 1558.71409,58.3562318 1562.44442,61.3515269 C1565.27245,63.6222848 1567.02271,66.5767393 1567.72914,70.1361141 C1567.83314,70.6605492 1567.87265,71.197715 1567.94398,71.7286436 C1567.95792,71.832625 1567.9811,71.9354102 1568,72.0387934 L1568,73.7951128 Z" id="GitHub"></path>
            </g>
        </g>
    </g>
</svg>

        <span class="nav_item_text">GitHub</span>
      </a>
    </div>
  </div>
</div>

    <div class="container content tutorial_container">
      <div class="vw_breadcrumb">
        <a class="home" href="/vowpalwabbit.github.io">
          Home
        </a>
        <span class="title">
          Get started with Vowpal Wabbit
        </span>
      </div>
      <a
        href="/blob/master/_guides/getting_started.md"
        class="edit"
        title="Edit this page"
        target="_blank"
      >
        <i class="fa fa-pencil-square-o fa-lg" aria-hidden="true"></i>
      </a>
      <h1 id="a-step-by-step-introduction-to-linear-regression">A Step by Step Introduction to Linear Regression</h1>

<h2 id="a-first-data-set">A first data-set</h2>

<p>Now, let’s create a data-set. Suppose we want to predict whether a house will require a new roof in the next 10 years. We can create a training-set file, <code class="highlighter-rouge">house_dataset</code> with the following contents:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 | price:.23 sqft:.25 age:.05 2006
1 2 'second_house | price:.18 sqft:.15 age:.35 1976
0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924
</code></pre></div></div>

<p>There is quite a bit going on here. The first number in each line is a label. A <code class="highlighter-rouge">0</code> label corresponds to no roof-replacement, while a <code class="highlighter-rouge">1</code> label corresponds to a roof-replacement. The bar <code class="highlighter-rouge">|</code> separates label related data (what we want to predict) from features (what we always know). The features in the 1st line are <code class="highlighter-rouge">price</code>, <code class="highlighter-rouge">sqft</code>, <code class="highlighter-rouge">age</code>, and <code class="highlighter-rouge">2006</code>. Each feature may have an optional <code class="highlighter-rouge">:&lt;numeric_value&gt;</code> following it or, if the value is missing, an implied value of <code class="highlighter-rouge">1</code>. By default, Vowpal Wabbit hashes feature names into in-memory indexes unless the feature names themselves are positive integers. In this case, the first 3 features use an index derived from a hash function while the last feature uses index 2006 directly. Also the 1st 3 features have explicit values (<code class="highlighter-rouge">.23</code>, <code class="highlighter-rouge">.25</code>, and <code class="highlighter-rouge">.05</code> respectively) while the last, <code class="highlighter-rouge">2006</code> has an implicit default value of 1.</p>

<p>The next example, on the next line, is similar, but the label information is more complex. The <code class="highlighter-rouge">1</code> is the label indicating that a roof-replacement is required. The <code class="highlighter-rouge">2</code> is an optional importance weight which implies that this example counts twice. Importance weights come up in many settings. A missing importance weight defaults to 1. <code class="highlighter-rouge">'second_house</code> is the tag, it is used elsewhere to identify the example.</p>

<p>The 3rd example is straightforward, except there is an additional number: <code class="highlighter-rouge">0.5</code> following the importance weight, in the label information. This is an initial prediction. Sometimes you have multiple interacting learning systems and want to be able to predict an offset rather than an absolute value.</p>

<p>Next, we learn:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vw house_dataset
</code></pre></div></div>

<p>Output:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = house_dataset
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0   0.0000   0.0000        5
0.666667 1.000000            2            3.0   1.0000   0.0000        5

finished run
number of examples = 3
weighted example sum = 4.000000
weighted label sum = 2.000000
average loss = 0.750000
best constant = 0.500000
best constant's loss = 0.250000
total feature number = 15
</code></pre></div></div>
<h2 id="vws-diagnostic-information">VW’s diagnostic information</h2>

<p>There is a burble of diagnostic information which you can turn off with <code class="highlighter-rouge">--quiet</code>. However, it’s worthwhile to first understand it, so let’s work through it bit by bit.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Num weight bits = 18
</code></pre></div></div>
<p>Only 18 bits of the hash function will be used. That’s much more than necessary for this example. You could adjust the number of bits using <code class="highlighter-rouge">-b bits</code></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learning rate = 0.5
</code></pre></div></div>
<p>The default learning rate is 0.5 which we found to be a good default with the current default update (<code class="highlighter-rouge">--normalized --invariant --adaptive</code>). If the data is noisy you’ll need a larger data-set and/or multiple passes to predict well. On these larger data-sets, our learning rate will by default decay towards 0 as we run through examples. You can adjust the learning rate up or down with <code class="highlighter-rouge">-l rate</code>. A higher learning rate will make the model converge faster but a too high learning rate may over-fit and end-up worse on average.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>initial_t = 0
</code></pre></div></div>
<p>Learning rates should often decay over time, and this specifies the initial time. You can adjust with <code class="highlighter-rouge">--initial_t time</code>, although this is rarely necessary these days.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>power_t = 0.5
</code></pre></div></div>
<p>This specifies the power on the learning rate decay. You can adjust this <code class="highlighter-rouge">--power_t p</code> where <em>p</em> is in the range [0,1]. 0 means the learning rate does not decay, which can be helpful when state tracking, while 1 is very aggressive, but plausibly optimal for <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">IID</a> data-sets. 0.5 is a minimax optimal choice. A different way of stating this is: stationary data-sets where the fundamental relation between the input features and target label are not changing over time, should benefit from a high (close to 1.0) <code class="highlighter-rouge">--power_t</code> while learning against changing conditions, like learning against an adversary who continuously changes the rules-of-the-game, would benefit from low (close to 0) <code class="highlighter-rouge">--power_t</code> so the learner can react quickly to these changing conditions. For many problems, 0.5, which is the default, seems to work best.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>using no cache
</code></pre></div></div>
<p>This says you are not using a cache. If you use multiple passes with <code class="highlighter-rouge">--passes</code>, you would need to also pass <code class="highlighter-rouge">-c</code> so VW can cache the data in a faster to handle format (passes &gt; 1 should be much faster). By default, the cache file name will be the data-set file with <code class="highlighter-rouge">.cache</code> appended. In this case: <code class="highlighter-rouge">house_dataset.cache</code>. You may also override the default cache file name by passing: <code class="highlighter-rouge">--cache_file housing.cache</code>. A cache file can greatly speed up training when you run many experiments (with different options) on the same data-set even if each experiment is only a single pass. So if you want to experiment with the same data-set over and over, it is highly recommended to pass <code class="highlighter-rouge">-c</code> every time you train. If the cache exists and is newer than the data-set, it will be used, if it doesn’t exist, it’ll be created the first time <code class="highlighter-rouge">-c</code> is used.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Reading datafile = house_dataset
</code></pre></div></div>
<p>There are many different ways to input data to VW. Here we’re just using a simple text file and VW tells us the source of the data. Alternative sources include cache files (from previous VW runs), stdin, or a tcp socket.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num sources = 1
</code></pre></div></div>
<p>There is only one input file in our example. But you can specify multiple files.</p>

<p>Next, there is a bunch of header information. VW is going to print out some live diagnostic information.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0   0.0000   0.0000        5
0.666667 1.000000            2            3.0   1.0000   0.0000        5
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">average loss</code> computes the <a href="http://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf" target="_blank">progressive validation</a> loss. The critical thing to understand here is that progressive validation loss deviates like a test set, and hence is a reliable indicator of success on the first pass over any data-set.</li>
  <li><code class="highlighter-rouge">since last</code> is the progressive validation loss since the last printout.</li>
  <li><code class="highlighter-rouge">example counter</code> tells you which example is printed out. In this case, it’s example 2.</li>
  <li><code class="highlighter-rouge">example weight</code> tells you the sum of the importance weights of examples seen so far. In this case it’s 3, because the second example has an importance weight of 2.</li>
  <li><code class="highlighter-rouge">current label</code> tells you the label of the second example.</li>
  <li><code class="highlighter-rouge">current predict</code> tells you the prediction (before training) on the current example.</li>
  <li><code class="highlighter-rouge">current features</code> tells you how many features the current example has. This is great for debugging, and you’ll note that we have 5 features when you expect 4. This happens, because VW has a default constant feature which is always added in. Use the <code class="highlighter-rouge">--noconstant</code> command-line option to turn it off.</li>
</ul>

<p>VW prints a new line with an exponential backoff. This is very handy, because often you can debug some problem before the learning algorithm finishes going through a data-set.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>finished run
number of examples = 3
weighted example sum = 4.000000
weighted label sum = 2.000000
average loss = 0.750000
best constant = 0.500000
best constant's loss = 0.250000
total feature number = 15
</code></pre></div></div>
<p>At the end, some more straightforward totals are printed. The only mysterious one is:
<code class="highlighter-rouge">best constant</code> and <code class="highlighter-rouge">best constant's loss</code> These really only work if you are using squared loss, which is the default. They compute the best constant’s predictor and the loss of the best constant predictor. If <code class="highlighter-rouge">average loss</code> is not better than <code class="highlighter-rouge">best constant's loss</code>, something is wrong. In this case, we have too few examples to generalize.</p>

<p>If we want to overfit like mad, we can simply use:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vw house_dataset <span class="nt">-l</span> 10 <span class="nt">-c</span> <span class="nt">--passes</span> 25 <span class="nt">--holdout_off</span>
</code></pre></div></div>

<p>The progress section of the output is:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0   0.0000   0.0000        5
0.666667 1.000000            2            3.0   1.0000   0.0000        5
0.589385 0.531424            5            7.0   1.0000   0.2508        5
0.378923 0.194769           11           15.0   1.0000   0.8308        5
0.184476 0.002182           23           31.0   1.0000   0.9975        5
0.090774 0.000000           47           63.0   1.0000   1.0000        5
</code></pre></div></div>

<p>You’ll notice that by example 47 (25 passes over 3 examples result in 75 examples), the <code class="highlighter-rouge">since last</code> column has dropped to 0, implying that by looking at the same (3 lines of) data 25 times we have reached a perfect predictor. This is unsurprising with 3 examples having 5 features each. The reason we have to add <code class="highlighter-rouge">--holdout_off</code> is that when running multiple-passes, VW automatically switches to ‘over-fit avoidance’ mode by holding-out 10% of the examples (the period “one in 10” can be changed using <code class="highlighter-rouge">--holdout_period period</code>) and evaluating performance on the held-out data instead of using the online-training progressive loss.</p>

<h2 id="saving-your-model-aka-regressor-into-a-file">Saving your model (a.k.a. regressor) into a file</h2>

<p>By default VW learns the weights of the features and keeps them in an in memory vector. If you want to save the final regressor weights into a file, add <code class="highlighter-rouge">-f filename</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vw house_dataset <span class="nt">-l</span> 10 <span class="nt">-c</span> <span class="nt">--passes</span> 25 <span class="nt">--holdout_off</span> <span class="nt">-f</span> house.model
</code></pre></div></div>

<h2 id="getting-predictions">Getting predictions</h2>

<p>We want to make predictions of course, this can be done by supplying the <code class="highlighter-rouge">-p filename</code> option. Stdout can be used as below:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vw house_dataset <span class="nt">-p</span> /dev/stdout <span class="nt">--quiet</span>
</code></pre></div></div>

<p>The output is:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.000000
0.000000 second_house
1.000000 third_house
</code></pre></div></div>

<ul>
  <li>The 1st output line <code class="highlighter-rouge">0.000000</code> is for the 1st example which has an empty tag.</li>
  <li>The 2nd output <code class="highlighter-rouge">0.000000 second_house</code> is for the 2nd example. You’ll notice the tag appears here. This is the primary use of the tag: mapping predictions to the examples they belong to.</li>
  <li>The 3rd output <code class="highlighter-rouge">1.000000 third_house</code> is for the 3rd example. Clearly, some learning happened, because the prediction is now <code class="highlighter-rouge">1.000000</code> while the initial prediction was set to <code class="highlighter-rouge">0.5</code>.</li>
</ul>

<p>Note that in this last example, we predicted <em>while we learned</em>. The model was being incrementally built in memory as VW went over the examples.</p>

<p>Alternatively, and more commonly, we would first learn and save the model into a file. Later we would predict using the saved model.</p>

<p>You may load a initial model to memory by adding <code class="highlighter-rouge">-i house.model</code>. You may also want to specify <code class="highlighter-rouge">-t</code> which stands for “test-only” (do no learning):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vw <span class="nt">-i</span> house.model <span class="nt">-t</span> house_dataset <span class="nt">-p</span> /dev/stdout <span class="nt">--quiet</span>
</code></pre></div></div>

<p>Which would output:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.000000
1.000000 second_house
0.000000 third_house
</code></pre></div></div>

<p>Obviously the results are different this time, because in the first prediction example, we learned as we went, and made only one pass over the data, whereas in the 2nd example we first loaded an over-fitted (25 pass) model and used our data-set <code class="highlighter-rouge">house_dataset</code> with <code class="highlighter-rouge">-t</code> (testing only mode). In real prediction settings, one should use a different data-set for testing vs training.</p>

<h2 id="auditing">Auditing</h2>

<p>When developing a new ML application, it’s very helpful to debug. VW can help with this using the <code class="highlighter-rouge">--audit</code> option, which outputs extra informations about predictions and features.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vw house_dataset <span class="nt">--audit</span> <span class="nt">--quiet</span>
</code></pre></div></div>

<p>Output:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0
  price:229902:0.23:0@0  sqft:162853:0.25:0@0  age:165201:0.05:0@0  2006:2006:1:0@0  Constant:116060:1:0@0
0 second_house
  price:229902:0.18:0@0  sqft:162853:0.15:0@0  age:165201:0.35:0@0  1976:1976:1:0@0  Constant:116060:1:0@0
1 third_house
  price:229902:0.53:0.882655@0.2592  age:165201:0.87:0.453833@0.98  sqft:162853:0.32:1.05905@0.18  Constant:116060:1:0.15882@8  1924:1924:1:0@0
</code></pre></div></div>

<p>Every example uses two lines. The first line has the prediction, and the second line has one entry per feature. Looking at the first feature, we see:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>price:229902:0.23:0@0.25
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">price</code> is the original feature name. If you use a namespace, it appears before <code class="highlighter-rouge">^</code> (i.e. <code class="highlighter-rouge">Namespace^Feature</code>). Namespaces are an advanced feature which allows you to group features and operate them on-the-fly, in the core of VW with the options: <code class="highlighter-rouge">-q XY</code> (cross a pair of namespaces), <code class="highlighter-rouge">--cubic XYZ</code> (cross 3 namespaces), <code class="highlighter-rouge">--lrq XYn</code> (low-rank quadratic interactions), and <code class="highlighter-rouge">--ignore X</code> (skip all features belonging to a namespace).</li>
  <li><code class="highlighter-rouge">229902</code> is the index of the feature, computed by a hash function on the feature name.</li>
  <li><code class="highlighter-rouge">0.23</code> is the value of the feature.</li>
  <li><code class="highlighter-rouge">0</code> is the value of the feature’s weight.</li>
  <li><code class="highlighter-rouge">@0.25</code> represents the sum of gradients squared for that feature when you are using per-feature adaptive learning rates.</li>
</ul>

<p>Examining further, you’ll notice that the feature <code class="highlighter-rouge">2006</code> uses the index 2006. This means that you may use hashes or pre-computed indices for features, as is common in other machine learning systems.</p>

<p>The advantage of using unique integer-based feature-names is that they are guaranteed not to collide after hashing. The advantage of free-text (non integer) feature names is readability and self-documentation. Since only <code class="highlighter-rouge">:</code>, <code class="highlighter-rouge">|</code>, and <em>spaces</em> are special to the VW parser, you can give features extremely readable names like: <code class="highlighter-rouge">height&gt;2 value_in_range[1..5] color=red</code> and so on. Feature names may even start with a digit, e.g.: <code class="highlighter-rouge">1st-guess:0.5 2nd-guess:3</code> etc.</p>

<h2 id="whats-next">What’s next?</h2>

<p>The above only scratches the surface of VW. You can learn with other loss functions, with other optimizers, with other representations, with clusters of 1000s of machines, and even do ridiculously fast active learning.</p>

<ul>
  <li>Learn about using VW to do reinforcement learning in the <a href="/vowpalwabbit.github.io/guides/contextual_bandits.html">Contextual Bandit tutorial</a></li>
  <li>Explore more content in the <a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Tutorial#more-tutorials" target="_blank">tutorials section of the GitHub wiki</a></li>
  <li>Browse <a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Examples" target="_blank">examples on the GitHub wiki</a></li>
</ul>

    </div>
    <div class="bibliography_container">
      <div class="container">
        <div class="row header">
          <div class="col-5">
            <h3>
              CITATIONS
            </h3>
          </div>
          <div class="col all_publications_link">
            <a href="/vowpalwabbit.github.io/publications">
              All publications
            </a>
          </div>
        </div>
        <div class="row">
          <div class="col-12">
            <ol class="bibliography"><li><span id="DBLP:journals/corr/AgarwalBCHLLLMO16">Agarwal, A., Bird, S., Cozowicz, M., Hoang, L., Langford, J., Lee, S., … Slivkins, A. (2016). A Multiworld Testing Decision Service. <i>CoRR</i>, <i>abs/1606.03966</i>. Retrieved from http://arxiv.org/abs/1606.03966</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_journals/corr/AgarwalBCHLLLMO16.html">Get .bib</a></li>
<li><span id="doi:10.1080/01621459.1952.10483446">Horvitz, D. G., &amp; Thompson, D. J. (1952). A Generalization of Sampling Without Replacement from a Finite Universe. <i>Journal of the American Statistical Association</i>, <i>47</i>(260), 663–685. https://doi.org/10.1080/01621459.1952.10483446</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/doi_10.1080/01621459.1952.10483446.html">Get .bib</a></li>
<li><span id="DBLP:conf/icml/JiangL16">Jiang, N., &amp; Li, L. (2016). Doubly Robust Off-policy Value Evaluation for Reinforcement Learning. In <i>Proceedings of the 33nd International Conference on Machine Learning,
               ICML 2016, New York City, NY, USA, June 19-24, 2016</i> (pp. 652–661). Retrieved from http://proceedings.mlr.press/v48/jiang16.html</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_conf/icml/JiangL16.html">Get .bib</a></li>
<li><span id="DBLP:conf/icml/DudikLL11">Dudı́k Miroslav, Langford, J., &amp; Li, L. (2011). Doubly Robust Policy Evaluation and Learning. In <i>Proceedings of the 28th International Conference on Machine Learning,
               ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011</i> (pp. 1097–1104). Retrieved from https://icml.cc/2011/papers/554_icmlpaper.pdf</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_conf/icml/DudikLL11.html">Get .bib</a></li>
<li><span id="bietti2018a">Bietti, A., Agarwal, A., &amp; Langford, J. (2018). A Contextual Bandit Bake-off. arXiv:1802.04064v3 [stat.ML]. Retrieved from https://www.microsoft.com/en-us/research/publication/a-contextual-bandit-bake-off-2/</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/bietti2018a.html">Get .bib</a></li>
<li><span id="Karampatziakis:2011:OIW:3020548.3020594">Karampatziakis, N., &amp; Langford, J. (2011). Online Importance Weight Aware Updates. In <i>Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</i> (pp. 392–399). Arlington, Virginia, United States: AUAI Press. Retrieved from http://dl.acm.org/citation.cfm?id=3020548.3020594</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/Karampatziakis_2011_OIW_3020548.3020594.html">Get .bib</a></li>
<li><span id="DBLP:journals/corr/abs-1202-1334">Agarwal, A., Dudı́k Miroslav, Kale, S., Langford, J., &amp; Schapire, R. E. (2012). Contextual Bandit Learning with Predictable Rewards. <i>CoRR</i>, <i>abs/1202.1334</i>. Retrieved from http://arxiv.org/abs/1202.1334</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_journals/corr/abs-1202-1334.html">Get .bib</a></li>
<li><span id="DBLP:journals/corr/EcklesK14">Eckles, D., &amp; Kaptein, M. (2014). Thompson sampling with the online bootstrap. <i>CoRR</i>, <i>abs/1410.4009</i>. Retrieved from http://arxiv.org/abs/1410.4009</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_journals/corr/EcklesK14.html">Get .bib</a></li>
<li><span id="DBLP:journals/corr/AgarwalHKLLS14">Agarwal, A., Hsu, D. J., Kale, S., Langford, J., Li, L., &amp; Schapire, R. E. (2014). Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits. <i>CoRR</i>, <i>abs/1402.0555</i>. Retrieved from http://arxiv.org/abs/1402.0555</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_journals/corr/AgarwalHKLLS14.html">Get .bib</a></li>
<li><span id="DBLP:journals/corr/abs-1811-04383">Cortes, D. (2018). Adapting multi-armed bandits policies to contextual bandits scenarios. <i>CoRR</i>, <i>abs/1811.04383</i>. Retrieved from http://arxiv.org/abs/1811.04383</span>
<a class="details" href="/vowpalwabbit.github.io/bibliography/DBLP_journals/corr/abs-1811-04383.html">Get .bib</a></li></ol>
          </div>
        </div>
      </div>
    </div>
    <div class="container-fluid footer_container">
  <div class="container">
    <div class="row align-items-center">
      <div class="col">
        <span class="title">Sponsored by</span>
        <img class="microsoft_logo" src="/vowpalwabbit.github.io/assets/images/microsoft_logo.svg" />
      </div>
    </div>
  </div>
</div>


    <div class="overlay">
    </div>
  </body>
</html>
