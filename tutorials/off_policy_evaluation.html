<!doctype html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.png">
  <link rel="canonical" href="/tutorials/off_policy_evaluation.html" />
  <link rel="stylesheet" href="/assets/bootstrap-4.1.3-dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="/assets/syntax.css">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="https://fonts.googleapis.com/css?family=Work+Sans:300,400,600&display=swap" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
  <script src="/assets/app.js"></script>

  <title>Offline Policy Evaluation of Contextual Bandits | Vowpal Wabbit</title>
  <meta name="description" content="Learn how to evaluate contextual bandit policies offline in Vowpal Wabbit." >
</head>


  <body class="tutorial">
    


<div class="navbar_container  main_nav_container">
  <div class="container navbar">
    <div class="logo">
      <a href="/index.html">
        <?xml version="1.0" encoding="UTF-8"?>
<svg width="172px" height="30px" viewBox="0 0 172 30" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 57.1 (83088) - https://sketch.com -->
    <title>logo_vw_horiz_gray</title>
    <desc>Created with Sketch.</desc>
    <g id="Styles" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Desktop-Copy" transform="translate(-944.000000, -314.000000)">
            <g id="logo_vw_horiz_gray" transform="translate(941.000000, 314.000000)">
                <rect id="framework" x="0" y="0" width="175" height="30"></rect>
                <path d="M49.488,22 L46.59,22 L42.252,10.12 L44.88,10.12 L47.382,17.68 L48.066,20.02 L48.732,17.698 L51.252,10.12 L53.808,10.12 L49.488,22 Z M58.524,12.82 C59.9760073,12.82 61.106996,13.2339959 61.917,14.062 C62.7270041,14.8900041 63.132,16.0359927 63.132,17.5 C63.132,18.9640073 62.7270041,20.1099959 61.917,20.938 C61.106996,21.7660041 59.9760073,22.18 58.524,22.18 C57.0719927,22.18 55.9410041,21.7660041 55.131,20.938 C54.320996,20.1099959 53.916,18.9640073 53.916,17.5 C53.916,16.0359927 54.320996,14.8900041 55.131,14.062 C55.9410041,13.2339959 57.0719927,12.82 58.524,12.82 Z M58.524,14.656 C57.8279965,14.656 57.2970018,14.8929976 56.931,15.367 C56.5649982,15.8410024 56.382,16.5519953 56.382,17.5 C56.382,18.4480047 56.5649982,19.1589976 56.931,19.633 C57.2970018,20.1070024 57.8279965,20.344 58.524,20.344 C59.2200035,20.344 59.7509982,20.1070024 60.117,19.633 C60.4830018,19.1589976 60.666,18.4480047 60.666,17.5 C60.666,16.5519953 60.4830018,15.8410024 60.117,15.367 C59.7509982,14.8929976 59.2200035,14.656 58.524,14.656 Z M79.368,13 L76.506,22 L73.644,22 L71.682,15.646 L69.792,22 L66.948,22 L64.086,13 L66.696,13 L68.496,20.02 L70.512,13 L72.942,13 L74.958,20.02 L76.758,13 L79.368,13 Z M86.316,12.82 C87.516006,12.82 88.4579966,13.2309959 89.142,14.053 C89.8260034,14.8750041 90.168,16.0179927 90.168,17.482 C90.168,18.9580074 89.8260034,20.1099959 89.142,20.938 C88.4579966,21.7660041 87.516006,22.18 86.316,22.18 C85.6679968,22.18 85.1040024,22.0330015 84.624,21.739 C84.1439976,21.4449985 83.7660014,21.0400026 83.49,20.524 L83.49,25.78 L81.024,25.78 L81.024,13 L83.202,13 L83.292,14.782 C83.5560013,14.181997 83.9489974,13.7050018 84.471,13.351 C84.9930026,12.9969982 85.6079965,12.82 86.316,12.82 Z M85.578,20.29 C86.2500034,20.29 86.7719981,20.0500024 87.144,19.57 C87.5160019,19.0899976 87.702,18.4000045 87.702,17.5 C87.702,16.5999955 87.5160019,15.9100024 87.144,15.43 C86.7719981,14.9499976 86.2500034,14.71 85.578,14.71 C84.977997,14.71 84.4920019,14.9169979 84.12,15.331 C83.7479981,15.7450021 83.5380002,16.3359962 83.49,17.104 L83.49,17.896 C83.5380002,18.6520038 83.7479981,19.2399979 84.12,19.66 C84.4920019,20.0800021 84.977997,20.29 85.578,20.29 Z M100.518,20.506 C100.770001,20.506 100.973999,20.4820002 101.13,20.434 L100.95,21.928 C100.613998,22.0960008 100.224002,22.18 99.78,22.18 C98.6279942,22.18 97.9380011,21.7300045 97.71,20.83 C97.4459987,21.2740022 97.0380028,21.6099989 96.486,21.838 C95.9339972,22.0660011 95.2800038,22.18 94.524,22.18 C93.6479956,22.18 92.9640025,21.982002 92.472,21.586 C91.9799975,21.189998 91.734,20.6200037 91.734,19.876 C91.734,18.4119927 92.9039883,17.4640022 95.244,17.032 L97.404,16.618 L97.404,16.186 C97.404,15.7059976 97.2690014,15.3250014 96.999,15.043 C96.7289987,14.7609986 96.3540024,14.62 95.874,14.62 C95.3219972,14.62 94.8720017,14.7399988 94.524,14.98 C94.1759983,15.2200012 93.9360007,15.6099973 93.804,16.15 L91.842,15.322 C92.034001,14.5419961 92.4659966,13.9300022 93.138,13.486 C93.8100034,13.0419978 94.6859946,12.82 95.766,12.82 C97.0500064,12.82 98.0519964,13.0989972 98.772,13.657 C99.4920036,14.2150028 99.852,15.0399945 99.852,16.132 L99.852,19.822 C99.852,20.0620012 99.9059995,20.2359995 100.014,20.344 C100.122001,20.4520005 100.289999,20.506 100.518,20.506 Z M95.298,20.506 C95.8260026,20.506 96.3089978,20.3770013 96.747,20.119 C97.1850022,19.8609987 97.404,19.5100022 97.404,19.066 L97.404,18.13 L95.478,18.562 C95.069998,18.6580005 94.767001,18.7869992 94.569,18.949 C94.370999,19.1110008 94.272,19.3359986 94.272,19.624 C94.272,19.9120014 94.3619991,20.1309992 94.542,20.281 C94.7220009,20.4310007 94.9739984,20.506 95.298,20.506 Z M105.054,19.138 C105.054,19.5700022 105.116999,19.8699992 105.243,20.038 C105.369001,20.2060008 105.599998,20.29 105.936,20.29 C106.140001,20.29 106.316999,20.2750001 106.467,20.245 C106.617001,20.2149998 106.793999,20.1640004 106.998,20.092 L106.782,21.802 C106.589999,21.9220006 106.338002,22.0149997 106.026,22.081 C105.713998,22.1470003 105.408002,22.18 105.108,22.18 C104.231996,22.18 103.593002,21.9670021 103.191,21.541 C102.788998,21.1149979 102.588,20.4340047 102.588,19.498 L102.588,9.058 L105.054,9.058 L105.054,19.138 Z M118.482,22 L115.62,22 L112.164,10.12 L114.846,10.12 L117.15,20.038 L119.526,10.12 L121.902,10.12 L124.314,20.038 L126.618,10.12 L129.174,10.12 L125.718,22 L122.928,22 L121.272,15.52 L120.714,12.712 L120.678,12.712 L120.12,15.52 L118.482,22 Z M138.336,20.506 C138.588001,20.506 138.791999,20.4820002 138.948,20.434 L138.768,21.928 C138.431998,22.0960008 138.042002,22.18 137.598,22.18 C136.445994,22.18 135.756001,21.7300045 135.528,20.83 C135.263999,21.2740022 134.856003,21.6099989 134.304,21.838 C133.751997,22.0660011 133.098004,22.18 132.342,22.18 C131.465996,22.18 130.782002,21.982002 130.29,21.586 C129.797998,21.189998 129.552,20.6200037 129.552,19.876 C129.552,18.4119927 130.721988,17.4640022 133.062,17.032 L135.222,16.618 L135.222,16.186 C135.222,15.7059976 135.087001,15.3250014 134.817,15.043 C134.546999,14.7609986 134.172002,14.62 133.692,14.62 C133.139997,14.62 132.690002,14.7399988 132.342,14.98 C131.993998,15.2200012 131.754001,15.6099973 131.622,16.15 L129.66,15.322 C129.852001,14.5419961 130.283997,13.9300022 130.956,13.486 C131.628003,13.0419978 132.503995,12.82 133.584,12.82 C134.868006,12.82 135.869996,13.0989972 136.59,13.657 C137.310004,14.2150028 137.67,15.0399945 137.67,16.132 L137.67,19.822 C137.67,20.0620012 137.723999,20.2359995 137.832,20.344 C137.940001,20.4520005 138.107999,20.506 138.336,20.506 Z M133.116,20.506 C133.644003,20.506 134.126998,20.3770013 134.565,20.119 C135.003002,19.8609987 135.222,19.5100022 135.222,19.066 L135.222,18.13 L133.296,18.562 C132.887998,18.6580005 132.585001,18.7869992 132.387,18.949 C132.188999,19.1110008 132.09,19.3359986 132.09,19.624 C132.09,19.9120014 132.179999,20.1309992 132.36,20.281 C132.540001,20.4310007 132.791998,20.506 133.116,20.506 Z M145.734,12.82 C146.934006,12.82 147.875997,13.2339959 148.56,14.062 C149.244003,14.8900041 149.586,16.0419926 149.586,17.518 C149.586,18.9820073 149.244003,20.1249959 148.56,20.947 C147.875997,21.7690041 146.934006,22.18 145.734,22.18 C145.025996,22.18 144.411003,22.0030018 143.889,21.649 C143.366997,21.2949982 142.974001,20.818003 142.71,20.218 L142.62,22 L140.442,22 L140.442,9.058 L142.908,9.058 L142.908,14.476 C143.184001,13.9599974 143.561998,13.5550015 144.042,13.261 C144.522002,12.9669985 145.085997,12.82 145.734,12.82 Z M144.996,20.29 C145.668003,20.29 146.189998,20.0500024 146.562,19.57 C146.934002,19.0899976 147.12,18.4000045 147.12,17.5 C147.12,16.5999955 146.934002,15.9100024 146.562,15.43 C146.189998,14.9499976 145.668003,14.71 144.996,14.71 C144.395997,14.71 143.910002,14.9199979 143.538,15.34 C143.165998,15.7600021 142.956,16.3479962 142.908,17.104 L142.908,17.896 C142.956,18.6640038 143.165998,19.2549979 143.538,19.669 C143.910002,20.0830021 144.395997,20.29 144.996,20.29 Z M157.056,12.82 C158.256006,12.82 159.197997,13.2339959 159.882,14.062 C160.566003,14.8900041 160.908,16.0419926 160.908,17.518 C160.908,18.9820073 160.566003,20.1249959 159.882,20.947 C159.197997,21.7690041 158.256006,22.18 157.056,22.18 C156.347996,22.18 155.733003,22.0030018 155.211,21.649 C154.688997,21.2949982 154.296001,20.818003 154.032,20.218 L153.942,22 L151.764,22 L151.764,9.058 L154.23,9.058 L154.23,14.476 C154.506001,13.9599974 154.883998,13.5550015 155.364,13.261 C155.844002,12.9669985 156.407997,12.82 157.056,12.82 Z M156.318,20.29 C156.990003,20.29 157.511998,20.0500024 157.884,19.57 C158.256002,19.0899976 158.442,18.4000045 158.442,17.5 C158.442,16.5999955 158.256002,15.9100024 157.884,15.43 C157.511998,14.9499976 156.990003,14.71 156.318,14.71 C155.717997,14.71 155.232002,14.9199979 154.86,15.34 C154.487998,15.7600021 154.278,16.3479962 154.23,17.104 L154.23,17.896 C154.278,18.6640038 154.487998,19.2549979 154.86,19.669 C155.232002,20.0830021 155.717997,20.29 156.318,20.29 Z M164.328,11.704 C163.307995,11.704 162.798,11.2660044 162.798,10.39 C162.798,9.50199556 163.307995,9.058 164.328,9.058 C165.348005,9.058 165.858,9.50199556 165.858,10.39 C165.858,11.2660044 165.348005,11.704 164.328,11.704 Z M165.552,22 L163.086,22 L163.086,13 L165.552,13 L165.552,22 Z M174.336,21.442 C174.047999,21.6700011 173.685002,21.8499993 173.247,21.982 C172.808998,22.1140007 172.350002,22.18 171.87,22.18 C169.86599,22.18 168.864,21.2740091 168.864,19.462 L168.864,14.836 L167.226,14.836 L167.226,13 L168.864,13 L168.864,10.93 L171.33,10.246 L171.33,13 L174.246,13 L174.246,14.836 L171.33,14.836 L171.33,19.066 C171.33,19.8940041 171.743996,20.308 172.572,20.308 C173.100003,20.308 173.561998,20.1400017 173.958,19.804 L174.336,21.442 Z" id="VowpalWabbit" fill="#333333" fill-rule="nonzero"></path>
                <g id="logo_vw_color" transform="translate(3.000000, 0.000000)" fill="#2A3B93">
                    <path d="M27.9875518,16.6117194 C27.9875518,9.21947752 21.9666728,9.36035971 21.9666728,9.36035971 C21.3736977,6.15536974 20.3362508,3.5963294 19.3414955,1.96947553 C18.1790857,0.436543115 17.3241355,0 17.3241355,0 C16.2887633,0.700737156 15.7766231,3.93943024 15.7766231,3.93943024 C14.1726939,1.18184505 12.329851,1.05294263 12.329851,1.05294263 C9.93824094,8.37466357 17.0427696,11.1203489 17.0427696,11.1203489 C9.97542657,11.1203489 6.33307087,15.0400524 4.59284756,17.9877167 C4.0042614,19.0524794 3.60319929,20.0302689 3.35048069,20.7371557 L2.99801525,21.8605394 C1.89952322,21.2108361 0.011355398,21.1878349 0.011355398,21.1878349 C-0.278708419,29.2136474 5.0845596,28.0167877 5.0845596,28.0167877 C5.0845596,29.1432063 6.56177043,30 6.56177043,30 L18.0978518,30 C18.0978518,30 17.9248509,28.5478912 16.7183131,27.7616983 C16.5196972,27.4693917 16.4533057,27.212705 16.6897456,26.9422016 C17.354698,26.1816454 17.8296525,27.1798804 17.8296525,27.1798804 L18.3302221,27.8553801 C19.7539686,29.627988 20.7440159,29.9516816 21.1646284,30 L21.391732,30 C20.5763614,27.5009384 20.6054875,25.3190215 20.6054875,25.3190215 C20.4822802,22.1365535 22.8107704,19.2166024 22.8107704,19.2166024 C28.5084697,19.2166024 27.9875518,16.6117194 27.9875518,16.6117194" id="logo_rabbit"></path>
                </g>
            </g>
        </g>
    </g>
</svg>
      </a>
    </div>

    <div class="nav">
      <a
        href="/start.html"
        class=''
      >
        Get started
      </a>
      <a
        href="/features.html"
        class=''
      >
        Features
      </a>
      <a
        href="/tutorials.html"
        class='active'
      >
        Tutorials
      </a>
      <a
        href="/blog.html"
        class=''
      >
        Blog
      </a>
      <a
        href="/research.html"
        class=''
      >
        Research
      </a>
      <div class="external_links">
        <a
          href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki"
          target="_blank"
        >
          Wiki
        </a>

        <a
          href="https://github.com/VowpalWabbit/vowpal_wabbit"
          target="_blank"
          class="github_link"
        >
          <?xml version="1.0" encoding="UTF-8"?>
<svg width="30px" height="30px" viewBox="0 0 30 30" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 53 (72520) - https://sketchapp.com -->
    <title>GitHub</title>
    <desc>Created with Sketch.</desc>
    <g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="HD_Desktop_header_Home" transform="translate(-1538.000000, -58.000000)" fill="#FFFFFF">
            <g id="Header">
                <path d="M1568,73.7951128 C1567.94603,74.2583723 1567.89891,74.7224863 1567.83682,75.1847206 C1567.56271,77.2238487 1566.86543,79.1138828 1565.7732,80.8526868 C1564.55268,82.7956941 1562.9969,84.4188114 1561.05088,85.6538569 C1560.03339,86.2996179 1558.94817,86.8066231 1557.80753,87.1895684 C1557.17559,87.4016322 1556.73848,87.0743943 1556.73531,86.4093237 C1556.72907,85.1020808 1556.73745,83.7947524 1556.73164,82.487424 C1556.72804,81.6908602 1556.64825,80.9063436 1556.21259,80.2087201 C1556.0855,80.0052003 1555.92463,79.8226135 1555.77393,79.6234512 C1556.35935,79.5140871 1556.9463,79.4362506 1557.51632,79.2916849 C1559.04696,78.9034423 1560.41543,78.2275208 1561.34791,76.8924242 C1562.03287,75.9115647 1562.34632,74.7851144 1562.48436,73.6108172 C1562.59169,72.6982249 1562.62222,71.7826422 1562.42971,70.8740656 C1562.23608,69.9599354 1561.84044,69.1418405 1561.22834,68.4359293 C1561.07756,68.2620575 1561.04763,68.1215929 1561.12221,67.896713 C1561.52468,66.6818314 1561.3562,65.491984 1560.9042,64.3262309 C1560.87829,64.2595017 1560.76574,64.1973008 1560.68569,64.1853391 C1560.15364,64.1058793 1559.65196,64.2559986 1559.18533,64.4761793 C1558.42904,64.8332361 1557.69764,65.242668 1556.9487,65.6159585 C1556.82229,65.6790137 1556.64782,65.7219904 1556.51706,65.6889249 C1554.16941,65.0964789 1551.82595,65.097077 1549.47805,65.6883268 C1549.34078,65.7228448 1549.14638,65.677134 1549.02331,65.5987849 C1548.07109,64.9919849 1547.06446,64.5120644 1545.97001,64.2236166 C1545.9511,64.218661 1545.93272,64.2115694 1545.91364,64.2077246 C1545.22551,64.0670892 1545.09971,64.1456947 1544.9036,64.8108506 C1544.58682,65.884755 1544.5296,66.9550708 1544.90668,68.0296587 C1544.93627,68.1139033 1544.90129,68.2616303 1544.8404,68.3306664 C1543.60397,69.7341156 1543.31096,71.3989279 1543.46559,73.1936101 C1543.55933,74.2817831 1543.81162,75.3262104 1544.29869,76.3083514 C1544.97176,77.6654063 1546.11181,78.4952066 1547.4926,78.982475 C1548.35956,79.2884382 1549.27852,79.4473579 1550.18824,79.6758264 C1550.19526,79.6319098 1550.20056,79.6666842 1550.18568,79.6820635 C1549.71726,80.1697591 1549.45556,80.755028 1549.34531,81.4170227 C1549.33137,81.5007546 1549.27365,81.6203716 1549.20685,81.6454912 C1548.16482,82.0372369 1547.11706,82.2433199 1546.08683,81.6149888 C1545.61867,81.3295314 1545.27683,80.9169382 1545.00101,80.4479541 C1544.56852,79.712395 1543.95864,79.179074 1543.13923,78.9361661 C1542.84434,78.8487602 1542.50823,78.8709748 1542.19359,78.8892591 C1541.97909,78.9018189 1541.90862,79.062362 1542.05256,79.2422147 C1542.18529,79.4077988 1542.32025,79.5950849 1542.4978,79.6971865 C1543.28625,80.1507913 1543.72798,80.8753286 1544.08881,81.6641173 C1544.30382,82.1341266 1544.48812,82.6140471 1544.88675,82.9753759 C1545.52169,83.5510755 1546.29115,83.7682658 1547.11612,83.8044072 C1547.81434,83.834995 1548.51496,83.8107298 1549.24722,83.8107298 C1549.24611,83.8009041 1549.26073,83.8742123 1549.26167,83.9476058 C1549.27211,84.7377616 1549.2828,85.5279173 1549.28844,86.3180731 C1549.294,87.0913115 1548.82935,87.4109452 1548.0952,87.1563319 C1544.28705,85.8355039 1541.4928,83.3342269 1539.62819,79.7992882 C1538.22405,77.1372118 1537.78027,74.2780237 1538.09739,71.2935795 C1538.4294,68.1688416 1539.66702,65.4337134 1541.73193,63.0872551 C1543.89022,60.6347648 1546.57714,59.0045559 1549.78825,58.3523869 C1554.45335,57.4048494 1558.71409,58.3562318 1562.44442,61.3515269 C1565.27245,63.6222848 1567.02271,66.5767393 1567.72914,70.1361141 C1567.83314,70.6605492 1567.87265,71.197715 1567.94398,71.7286436 C1567.95792,71.832625 1567.9811,71.9354102 1568,72.0387934 L1568,73.7951128 Z" id="GitHub"></path>
            </g>
        </g>
    </g>
</svg>
          <div>
            GitHub
          </div>
        </a>
      </div>
    </div>
  </div>
</div>




<div class="navbar_container  mobile_nav_container">
  <div class="container navbar">
    <div class="hamburger_icon">
      <?xml version="1.0" encoding="UTF-8"?>
<svg width="18px" height="16px" viewBox="0 0 18 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: sketchtool 57.1 (101010) - https://sketch.com -->
    <title>8049A6A1-2687-43E9-9B5D-E205E8F9E871</title>
    <desc>Created with sketchtool.</desc>
    <g id="Pages" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd" stroke-linecap="round">
        <g id="Mobile_home" transform="translate(-12.000000, -56.000000)" stroke="#FFFFFF" stroke-width="2">
            <g id="header">
                <g id="ui_header_mobile_dk" transform="translate(12.000000, 44.000000)">
                    <g id="icon_menu" transform="translate(1.000000, 12.500000)">
                        <g id="Line">
                            <path d="M16,0.5 L0,0.5"></path>
                            <path d="M16,7.5 L0,7.5"></path>
                            <path d="M16,14.5 L0,14.5"></path>
                        </g>
                    </g>
                </g>
            </g>
        </g>
    </g>
</svg>
    </div>
    <div class="logo">
      <a href="/index.html">
        <?xml version="1.0" encoding="UTF-8"?>
<svg width="172px" height="30px" viewBox="0 0 172 30" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 57.1 (83088) - https://sketch.com -->
    <title>logo_vw_horiz_gray</title>
    <desc>Created with Sketch.</desc>
    <g id="Styles" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Desktop-Copy" transform="translate(-944.000000, -314.000000)">
            <g id="logo_vw_horiz_gray" transform="translate(941.000000, 314.000000)">
                <rect id="framework" x="0" y="0" width="175" height="30"></rect>
                <path d="M49.488,22 L46.59,22 L42.252,10.12 L44.88,10.12 L47.382,17.68 L48.066,20.02 L48.732,17.698 L51.252,10.12 L53.808,10.12 L49.488,22 Z M58.524,12.82 C59.9760073,12.82 61.106996,13.2339959 61.917,14.062 C62.7270041,14.8900041 63.132,16.0359927 63.132,17.5 C63.132,18.9640073 62.7270041,20.1099959 61.917,20.938 C61.106996,21.7660041 59.9760073,22.18 58.524,22.18 C57.0719927,22.18 55.9410041,21.7660041 55.131,20.938 C54.320996,20.1099959 53.916,18.9640073 53.916,17.5 C53.916,16.0359927 54.320996,14.8900041 55.131,14.062 C55.9410041,13.2339959 57.0719927,12.82 58.524,12.82 Z M58.524,14.656 C57.8279965,14.656 57.2970018,14.8929976 56.931,15.367 C56.5649982,15.8410024 56.382,16.5519953 56.382,17.5 C56.382,18.4480047 56.5649982,19.1589976 56.931,19.633 C57.2970018,20.1070024 57.8279965,20.344 58.524,20.344 C59.2200035,20.344 59.7509982,20.1070024 60.117,19.633 C60.4830018,19.1589976 60.666,18.4480047 60.666,17.5 C60.666,16.5519953 60.4830018,15.8410024 60.117,15.367 C59.7509982,14.8929976 59.2200035,14.656 58.524,14.656 Z M79.368,13 L76.506,22 L73.644,22 L71.682,15.646 L69.792,22 L66.948,22 L64.086,13 L66.696,13 L68.496,20.02 L70.512,13 L72.942,13 L74.958,20.02 L76.758,13 L79.368,13 Z M86.316,12.82 C87.516006,12.82 88.4579966,13.2309959 89.142,14.053 C89.8260034,14.8750041 90.168,16.0179927 90.168,17.482 C90.168,18.9580074 89.8260034,20.1099959 89.142,20.938 C88.4579966,21.7660041 87.516006,22.18 86.316,22.18 C85.6679968,22.18 85.1040024,22.0330015 84.624,21.739 C84.1439976,21.4449985 83.7660014,21.0400026 83.49,20.524 L83.49,25.78 L81.024,25.78 L81.024,13 L83.202,13 L83.292,14.782 C83.5560013,14.181997 83.9489974,13.7050018 84.471,13.351 C84.9930026,12.9969982 85.6079965,12.82 86.316,12.82 Z M85.578,20.29 C86.2500034,20.29 86.7719981,20.0500024 87.144,19.57 C87.5160019,19.0899976 87.702,18.4000045 87.702,17.5 C87.702,16.5999955 87.5160019,15.9100024 87.144,15.43 C86.7719981,14.9499976 86.2500034,14.71 85.578,14.71 C84.977997,14.71 84.4920019,14.9169979 84.12,15.331 C83.7479981,15.7450021 83.5380002,16.3359962 83.49,17.104 L83.49,17.896 C83.5380002,18.6520038 83.7479981,19.2399979 84.12,19.66 C84.4920019,20.0800021 84.977997,20.29 85.578,20.29 Z M100.518,20.506 C100.770001,20.506 100.973999,20.4820002 101.13,20.434 L100.95,21.928 C100.613998,22.0960008 100.224002,22.18 99.78,22.18 C98.6279942,22.18 97.9380011,21.7300045 97.71,20.83 C97.4459987,21.2740022 97.0380028,21.6099989 96.486,21.838 C95.9339972,22.0660011 95.2800038,22.18 94.524,22.18 C93.6479956,22.18 92.9640025,21.982002 92.472,21.586 C91.9799975,21.189998 91.734,20.6200037 91.734,19.876 C91.734,18.4119927 92.9039883,17.4640022 95.244,17.032 L97.404,16.618 L97.404,16.186 C97.404,15.7059976 97.2690014,15.3250014 96.999,15.043 C96.7289987,14.7609986 96.3540024,14.62 95.874,14.62 C95.3219972,14.62 94.8720017,14.7399988 94.524,14.98 C94.1759983,15.2200012 93.9360007,15.6099973 93.804,16.15 L91.842,15.322 C92.034001,14.5419961 92.4659966,13.9300022 93.138,13.486 C93.8100034,13.0419978 94.6859946,12.82 95.766,12.82 C97.0500064,12.82 98.0519964,13.0989972 98.772,13.657 C99.4920036,14.2150028 99.852,15.0399945 99.852,16.132 L99.852,19.822 C99.852,20.0620012 99.9059995,20.2359995 100.014,20.344 C100.122001,20.4520005 100.289999,20.506 100.518,20.506 Z M95.298,20.506 C95.8260026,20.506 96.3089978,20.3770013 96.747,20.119 C97.1850022,19.8609987 97.404,19.5100022 97.404,19.066 L97.404,18.13 L95.478,18.562 C95.069998,18.6580005 94.767001,18.7869992 94.569,18.949 C94.370999,19.1110008 94.272,19.3359986 94.272,19.624 C94.272,19.9120014 94.3619991,20.1309992 94.542,20.281 C94.7220009,20.4310007 94.9739984,20.506 95.298,20.506 Z M105.054,19.138 C105.054,19.5700022 105.116999,19.8699992 105.243,20.038 C105.369001,20.2060008 105.599998,20.29 105.936,20.29 C106.140001,20.29 106.316999,20.2750001 106.467,20.245 C106.617001,20.2149998 106.793999,20.1640004 106.998,20.092 L106.782,21.802 C106.589999,21.9220006 106.338002,22.0149997 106.026,22.081 C105.713998,22.1470003 105.408002,22.18 105.108,22.18 C104.231996,22.18 103.593002,21.9670021 103.191,21.541 C102.788998,21.1149979 102.588,20.4340047 102.588,19.498 L102.588,9.058 L105.054,9.058 L105.054,19.138 Z M118.482,22 L115.62,22 L112.164,10.12 L114.846,10.12 L117.15,20.038 L119.526,10.12 L121.902,10.12 L124.314,20.038 L126.618,10.12 L129.174,10.12 L125.718,22 L122.928,22 L121.272,15.52 L120.714,12.712 L120.678,12.712 L120.12,15.52 L118.482,22 Z M138.336,20.506 C138.588001,20.506 138.791999,20.4820002 138.948,20.434 L138.768,21.928 C138.431998,22.0960008 138.042002,22.18 137.598,22.18 C136.445994,22.18 135.756001,21.7300045 135.528,20.83 C135.263999,21.2740022 134.856003,21.6099989 134.304,21.838 C133.751997,22.0660011 133.098004,22.18 132.342,22.18 C131.465996,22.18 130.782002,21.982002 130.29,21.586 C129.797998,21.189998 129.552,20.6200037 129.552,19.876 C129.552,18.4119927 130.721988,17.4640022 133.062,17.032 L135.222,16.618 L135.222,16.186 C135.222,15.7059976 135.087001,15.3250014 134.817,15.043 C134.546999,14.7609986 134.172002,14.62 133.692,14.62 C133.139997,14.62 132.690002,14.7399988 132.342,14.98 C131.993998,15.2200012 131.754001,15.6099973 131.622,16.15 L129.66,15.322 C129.852001,14.5419961 130.283997,13.9300022 130.956,13.486 C131.628003,13.0419978 132.503995,12.82 133.584,12.82 C134.868006,12.82 135.869996,13.0989972 136.59,13.657 C137.310004,14.2150028 137.67,15.0399945 137.67,16.132 L137.67,19.822 C137.67,20.0620012 137.723999,20.2359995 137.832,20.344 C137.940001,20.4520005 138.107999,20.506 138.336,20.506 Z M133.116,20.506 C133.644003,20.506 134.126998,20.3770013 134.565,20.119 C135.003002,19.8609987 135.222,19.5100022 135.222,19.066 L135.222,18.13 L133.296,18.562 C132.887998,18.6580005 132.585001,18.7869992 132.387,18.949 C132.188999,19.1110008 132.09,19.3359986 132.09,19.624 C132.09,19.9120014 132.179999,20.1309992 132.36,20.281 C132.540001,20.4310007 132.791998,20.506 133.116,20.506 Z M145.734,12.82 C146.934006,12.82 147.875997,13.2339959 148.56,14.062 C149.244003,14.8900041 149.586,16.0419926 149.586,17.518 C149.586,18.9820073 149.244003,20.1249959 148.56,20.947 C147.875997,21.7690041 146.934006,22.18 145.734,22.18 C145.025996,22.18 144.411003,22.0030018 143.889,21.649 C143.366997,21.2949982 142.974001,20.818003 142.71,20.218 L142.62,22 L140.442,22 L140.442,9.058 L142.908,9.058 L142.908,14.476 C143.184001,13.9599974 143.561998,13.5550015 144.042,13.261 C144.522002,12.9669985 145.085997,12.82 145.734,12.82 Z M144.996,20.29 C145.668003,20.29 146.189998,20.0500024 146.562,19.57 C146.934002,19.0899976 147.12,18.4000045 147.12,17.5 C147.12,16.5999955 146.934002,15.9100024 146.562,15.43 C146.189998,14.9499976 145.668003,14.71 144.996,14.71 C144.395997,14.71 143.910002,14.9199979 143.538,15.34 C143.165998,15.7600021 142.956,16.3479962 142.908,17.104 L142.908,17.896 C142.956,18.6640038 143.165998,19.2549979 143.538,19.669 C143.910002,20.0830021 144.395997,20.29 144.996,20.29 Z M157.056,12.82 C158.256006,12.82 159.197997,13.2339959 159.882,14.062 C160.566003,14.8900041 160.908,16.0419926 160.908,17.518 C160.908,18.9820073 160.566003,20.1249959 159.882,20.947 C159.197997,21.7690041 158.256006,22.18 157.056,22.18 C156.347996,22.18 155.733003,22.0030018 155.211,21.649 C154.688997,21.2949982 154.296001,20.818003 154.032,20.218 L153.942,22 L151.764,22 L151.764,9.058 L154.23,9.058 L154.23,14.476 C154.506001,13.9599974 154.883998,13.5550015 155.364,13.261 C155.844002,12.9669985 156.407997,12.82 157.056,12.82 Z M156.318,20.29 C156.990003,20.29 157.511998,20.0500024 157.884,19.57 C158.256002,19.0899976 158.442,18.4000045 158.442,17.5 C158.442,16.5999955 158.256002,15.9100024 157.884,15.43 C157.511998,14.9499976 156.990003,14.71 156.318,14.71 C155.717997,14.71 155.232002,14.9199979 154.86,15.34 C154.487998,15.7600021 154.278,16.3479962 154.23,17.104 L154.23,17.896 C154.278,18.6640038 154.487998,19.2549979 154.86,19.669 C155.232002,20.0830021 155.717997,20.29 156.318,20.29 Z M164.328,11.704 C163.307995,11.704 162.798,11.2660044 162.798,10.39 C162.798,9.50199556 163.307995,9.058 164.328,9.058 C165.348005,9.058 165.858,9.50199556 165.858,10.39 C165.858,11.2660044 165.348005,11.704 164.328,11.704 Z M165.552,22 L163.086,22 L163.086,13 L165.552,13 L165.552,22 Z M174.336,21.442 C174.047999,21.6700011 173.685002,21.8499993 173.247,21.982 C172.808998,22.1140007 172.350002,22.18 171.87,22.18 C169.86599,22.18 168.864,21.2740091 168.864,19.462 L168.864,14.836 L167.226,14.836 L167.226,13 L168.864,13 L168.864,10.93 L171.33,10.246 L171.33,13 L174.246,13 L174.246,14.836 L171.33,14.836 L171.33,19.066 C171.33,19.8940041 171.743996,20.308 172.572,20.308 C173.100003,20.308 173.561998,20.1400017 173.958,19.804 L174.336,21.442 Z" id="VowpalWabbit" fill="#333333" fill-rule="nonzero"></path>
                <g id="logo_vw_color" transform="translate(3.000000, 0.000000)" fill="#2A3B93">
                    <path d="M27.9875518,16.6117194 C27.9875518,9.21947752 21.9666728,9.36035971 21.9666728,9.36035971 C21.3736977,6.15536974 20.3362508,3.5963294 19.3414955,1.96947553 C18.1790857,0.436543115 17.3241355,0 17.3241355,0 C16.2887633,0.700737156 15.7766231,3.93943024 15.7766231,3.93943024 C14.1726939,1.18184505 12.329851,1.05294263 12.329851,1.05294263 C9.93824094,8.37466357 17.0427696,11.1203489 17.0427696,11.1203489 C9.97542657,11.1203489 6.33307087,15.0400524 4.59284756,17.9877167 C4.0042614,19.0524794 3.60319929,20.0302689 3.35048069,20.7371557 L2.99801525,21.8605394 C1.89952322,21.2108361 0.011355398,21.1878349 0.011355398,21.1878349 C-0.278708419,29.2136474 5.0845596,28.0167877 5.0845596,28.0167877 C5.0845596,29.1432063 6.56177043,30 6.56177043,30 L18.0978518,30 C18.0978518,30 17.9248509,28.5478912 16.7183131,27.7616983 C16.5196972,27.4693917 16.4533057,27.212705 16.6897456,26.9422016 C17.354698,26.1816454 17.8296525,27.1798804 17.8296525,27.1798804 L18.3302221,27.8553801 C19.7539686,29.627988 20.7440159,29.9516816 21.1646284,30 L21.391732,30 C20.5763614,27.5009384 20.6054875,25.3190215 20.6054875,25.3190215 C20.4822802,22.1365535 22.8107704,19.2166024 22.8107704,19.2166024 C28.5084697,19.2166024 27.9875518,16.6117194 27.9875518,16.6117194" id="logo_rabbit"></path>
                </g>
            </g>
        </g>
    </g>
</svg>
      </a>
    </div>
  </div>
</div>

<div class="mobile_nav">
  <button type="button" class="go_back_button">
    <
  </button>

  <a href="/start.html" class=''>
    Get started
  </a>
  <a
    href="/features.html"
    class=''
  >
    Features
  </a>
  <a href="/tutorials.html" class='active'>
    Tutorials
  </a>
  <a
    href="/blog.html"
    class=''
  >
    Blog
  </a>
  <a href="/research.html" class=''>
    Research
  </a>
  <div class="external_links">
    <a
      href="https://github.com/VowpalWabbit/vowpalwabbit.github.io/issues/new"
      target="_blank"
    >
      Feedback
    </a>

    <a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki" target="_blank">
      Wiki
    </a>

    <a href="https://github.com/VowpalWabbit/vowpal_wabbit" target="_blank" class="github_link">
      <?xml version="1.0" encoding="UTF-8"?>
<svg width="30px" height="30px" viewBox="0 0 30 30" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 53 (72520) - https://sketchapp.com -->
    <title>GitHub</title>
    <desc>Created with Sketch.</desc>
    <g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="HD_Desktop_header_Home" transform="translate(-1538.000000, -58.000000)" fill="#FFFFFF">
            <g id="Header">
                <path d="M1568,73.7951128 C1567.94603,74.2583723 1567.89891,74.7224863 1567.83682,75.1847206 C1567.56271,77.2238487 1566.86543,79.1138828 1565.7732,80.8526868 C1564.55268,82.7956941 1562.9969,84.4188114 1561.05088,85.6538569 C1560.03339,86.2996179 1558.94817,86.8066231 1557.80753,87.1895684 C1557.17559,87.4016322 1556.73848,87.0743943 1556.73531,86.4093237 C1556.72907,85.1020808 1556.73745,83.7947524 1556.73164,82.487424 C1556.72804,81.6908602 1556.64825,80.9063436 1556.21259,80.2087201 C1556.0855,80.0052003 1555.92463,79.8226135 1555.77393,79.6234512 C1556.35935,79.5140871 1556.9463,79.4362506 1557.51632,79.2916849 C1559.04696,78.9034423 1560.41543,78.2275208 1561.34791,76.8924242 C1562.03287,75.9115647 1562.34632,74.7851144 1562.48436,73.6108172 C1562.59169,72.6982249 1562.62222,71.7826422 1562.42971,70.8740656 C1562.23608,69.9599354 1561.84044,69.1418405 1561.22834,68.4359293 C1561.07756,68.2620575 1561.04763,68.1215929 1561.12221,67.896713 C1561.52468,66.6818314 1561.3562,65.491984 1560.9042,64.3262309 C1560.87829,64.2595017 1560.76574,64.1973008 1560.68569,64.1853391 C1560.15364,64.1058793 1559.65196,64.2559986 1559.18533,64.4761793 C1558.42904,64.8332361 1557.69764,65.242668 1556.9487,65.6159585 C1556.82229,65.6790137 1556.64782,65.7219904 1556.51706,65.6889249 C1554.16941,65.0964789 1551.82595,65.097077 1549.47805,65.6883268 C1549.34078,65.7228448 1549.14638,65.677134 1549.02331,65.5987849 C1548.07109,64.9919849 1547.06446,64.5120644 1545.97001,64.2236166 C1545.9511,64.218661 1545.93272,64.2115694 1545.91364,64.2077246 C1545.22551,64.0670892 1545.09971,64.1456947 1544.9036,64.8108506 C1544.58682,65.884755 1544.5296,66.9550708 1544.90668,68.0296587 C1544.93627,68.1139033 1544.90129,68.2616303 1544.8404,68.3306664 C1543.60397,69.7341156 1543.31096,71.3989279 1543.46559,73.1936101 C1543.55933,74.2817831 1543.81162,75.3262104 1544.29869,76.3083514 C1544.97176,77.6654063 1546.11181,78.4952066 1547.4926,78.982475 C1548.35956,79.2884382 1549.27852,79.4473579 1550.18824,79.6758264 C1550.19526,79.6319098 1550.20056,79.6666842 1550.18568,79.6820635 C1549.71726,80.1697591 1549.45556,80.755028 1549.34531,81.4170227 C1549.33137,81.5007546 1549.27365,81.6203716 1549.20685,81.6454912 C1548.16482,82.0372369 1547.11706,82.2433199 1546.08683,81.6149888 C1545.61867,81.3295314 1545.27683,80.9169382 1545.00101,80.4479541 C1544.56852,79.712395 1543.95864,79.179074 1543.13923,78.9361661 C1542.84434,78.8487602 1542.50823,78.8709748 1542.19359,78.8892591 C1541.97909,78.9018189 1541.90862,79.062362 1542.05256,79.2422147 C1542.18529,79.4077988 1542.32025,79.5950849 1542.4978,79.6971865 C1543.28625,80.1507913 1543.72798,80.8753286 1544.08881,81.6641173 C1544.30382,82.1341266 1544.48812,82.6140471 1544.88675,82.9753759 C1545.52169,83.5510755 1546.29115,83.7682658 1547.11612,83.8044072 C1547.81434,83.834995 1548.51496,83.8107298 1549.24722,83.8107298 C1549.24611,83.8009041 1549.26073,83.8742123 1549.26167,83.9476058 C1549.27211,84.7377616 1549.2828,85.5279173 1549.28844,86.3180731 C1549.294,87.0913115 1548.82935,87.4109452 1548.0952,87.1563319 C1544.28705,85.8355039 1541.4928,83.3342269 1539.62819,79.7992882 C1538.22405,77.1372118 1537.78027,74.2780237 1538.09739,71.2935795 C1538.4294,68.1688416 1539.66702,65.4337134 1541.73193,63.0872551 C1543.89022,60.6347648 1546.57714,59.0045559 1549.78825,58.3523869 C1554.45335,57.4048494 1558.71409,58.3562318 1562.44442,61.3515269 C1565.27245,63.6222848 1567.02271,66.5767393 1567.72914,70.1361141 C1567.83314,70.6605492 1567.87265,71.197715 1567.94398,71.7286436 C1567.95792,71.832625 1567.9811,71.9354102 1568,72.0387934 L1568,73.7951128 Z" id="GitHub"></path>
            </g>
        </g>
    </g>
</svg>
      <div>
        GitHub
      </div>
    </a>
  </div>
</div>

<script src="/assets/js/header.js"></script>


    <div class="hero">
      <div class="container">
        <div class="row">
          <div class="col-lg-6">
            <h1>
              Tutorials
            </h1>
            <a href="/tutorials.html">
              < Back to all tutorials
            </a>
          </div>
        </div>
      </div>
    </div>

    <div class="container content tutorial_container">
      <div class="edit">
  
  

  <a href="https://github.com/VowpalWabbit/vowpalwabbit.github.io/blob/source/_tutorials/off_policy_evaluation.md" target="_blank">
    <?xml version="1.0" encoding="UTF-8"?>
<svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 57.1 (83088) - https://sketch.com -->
    <title>icon_edit</title>
    <desc>Created with Sketch.</desc>
    <g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="button_edit" transform="translate(-7.000000, -7.000000)" stroke="#2A3B93">
            <g id="icon_edit_selected">
                <g transform="translate(8.000000, 8.000000)">
                    <path d="M3.99393054,13.0502777 C0,14.1182229 0,14.1182229 0,14.1182229 C1.01972686,10.1881871 1.01972686,10.1881871 1.01972686,10.1881871 C10.579668,0.576689783 10.579668,0.576689783 10.579668,0.576689783 C11.3869519,-0.192229928 12.661609,-0.192229928 13.4264041,0.576689783 C14.1911991,1.34560988 14.1911991,2.62714261 13.4264041,3.43878036 L3.99393054,13.0502777 Z M9.85736179,1.30289223 C12.7465867,4.16498242 12.7465867,4.16498242 12.7465867,4.16498242 L9.85736179,1.30289223 Z M3.99393054,13.0502777 C3.99393054,13.0502777 4.16388585,12.0677717 3.14415784,10.9143911 C2.12443098,9.7610106 1.01972686,10.1881871 1.01972686,10.1881871" id="icon_edit"></path>
                </g>
            </g>
        </g>
    </g>
</svg>
    <div class="text">
      Edit on GitHub
    </div>
  </a>
</div>

      <div class="row">
        <div class="col-lg-3">
          <div class="tutorial_nav">
            <ul class="tutorial_content_nav">
              <li class="label">
                Table of contents
              </li>
            </ul>
            
          </div>
        </div>
        <div class="col-lg-9">
          <h1 id="offline-policy-evaluation-using-the-vw-command-line">Offline policy evaluation using the VW command line</h1>

<p>Offline policy evaluation (OPE) is an active area of research in reinforcement learning. The aim, in a contextual bandit setting, is to take bandit data generated by some policy (let’s call it the <em>production policy</em>) and estimate the value of a new <em>candidate policy</em> offline. The use case is clear: before you deploy a policy, you want to estimate its performance, and compare to what’s already deployed. This tutorial walks you through how to evaluate new policies in a variety of scenarios: in batch learning settings, where deployed policies stay the same until replaced; and in online settings, where you plan to deploy a policy that learns in real time as new data comes in.</p>

<p>This tutorial is text-heavy, but we encourage you to read through it carefully to make the most of OPE.</p>

<h2 id="introduction--motivation">Introduction &amp; Motivation</h2>

<p>In supervised learning settings, the standard approach to offline evaluation is to train on a train set and estimate generalisation performance on a holdout set. In online learning settings, one typically uses progressive validation. In contextual bandit settings, neither is directly possible, because like all reinforcement learning, there is a partial information problem: you never get to see rewards of actions you didn’t take. Your only source of information is the bandit data generated by your production policy, which might make entirely different choices than your candidate policy.</p>

<p>It doesn’t, then, seem possible to reliably evaluate contextual bandit policies offline. But it is! The key is to use estimators that fill in fake rewards for actions that weren’t taken, thereby creating a “fake” supervised learning dataset, against which you can estimate performance, either using progressive validation (more on that later) or a holdout set.</p>

<p>VW implements several estimators to reduce policy evaluation to supervised learning-type evaluation. The simplest method, the direct method (DM), simply trains a regression model that estimates the cost (negative reward) of an (action, context) pair. As you might suspect, this method is generally biased, because the partial information problem means you typically see many more rewards for good actions than bad ones (assuming your production policy is working normally). Biased estimators should not be used for offline policy evaluation, but VW implements provably unbiased estimators like inverse propensity weighting (IPS) and doubly robust (DR) that can be used for this purpose.</p>

<p>Finally, before we get into how to run offline policy evaluation in VW, note that in this tutorial, by policies we mean contextual bandit models, not the exploration layer (e.g. epsilon-greedy) that is usually part of a contextual bandit system to tackle the explore-exploit tradeoff.</p>

<p>For now, If you wish to evaluate the performance of the entire loop (model + exploration), please refer to the documentation for <code class="highlighter-rouge">--explore_eval</code>. It is useful if you want to understand how different types of exploration might lead to better future rewards in an online learning bandit system.</p>

<h2 id="batch-scenario-policy-evaluation-with-a-pre-trained-vw-policy-cb-format-data">Batch scenario: policy evaluation with a pre-trained VW policy, <code class="highlighter-rouge">cb</code>-format data</h2>

<p>Let’s say you have collected the following bandit data from your production policy, and that the data is ordered such that the oldest data is first (don’t worry about the actual numbers in this toy example):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1:1:0.5 | user_age:25
2:0:0.5 | user_age:25
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
1:1:0.5 | user_age:27
2:0:0.5 | user_age:21
2:0:0.5 | user_age:23
2:0:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:36
1:1:0.5 | user_age:25
2:0:0.5 | user_age:25
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
1:1:0.5 | user_age:27
2:0:0.5 | user_age:21
2:0:0.5 | user_age:23
2:0:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:36
</code></pre></div></div>

<p>In order to do OPE, it is useful to think carefully about what you wish to evaluate. In a batch setting, you are interested in training a candidate policy and evaluating its performance on unseen data that is fresher than what you trained on. So, let’s split our data into two files. Starting from the oldest data first, we do e.g. and 70%/30% split, and save the results as <code class="highlighter-rouge">train.dat</code> and <code class="highlighter-rouge">test.dat</code>:</p>

<p><code class="highlighter-rouge">train.dat</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1:1:0.5 | user_age:25
2:0:0.5 | user_age:25
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
1:1:0.5 | user_age:27
2:0:0.5 | user_age:21
2:0:0.5 | user_age:23
2:0:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:36
1:1:0.5 | user_age:25
2:0:0.5 | user_age:25
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
</code></pre></div></div>

<p><code class="highlighter-rouge">test.dat</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
1:1:0.5 | user_age:27
2:0:0.5 | user_age:21
2:0:0.5 | user_age:23
2:0:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:36
</code></pre></div></div>

<p>Before continuing, it is worth understanding that policy value estimators such as IPS, DM and DR aren’t only useful for policy value estimation. Since they provide us a way to fill in fake rewards for untaken actions, they allow us to reduce bandit learning to supervised learning, and used to <em>train</em> policies. For example, say you have a (biased) DM estimator. For each untaken action per round, you can predict a reward, thus forming a supervised learning example where the loss of each action is known (estimated). You can then train an importance-weighted classification model, or even a regression model that estimates costs of arms given contexts, and use these models as policies. This is, in fact, what VW does: estimators serve a dual purpose and are used not only for evaluation, but also optimisation/training.</p>

<p>Now that we know that the same estimators that are used for OPE can also be used to train policies, let’s train a new candidate policy on the train set using e.g. IPS, and save the model as <code class="highlighter-rouge">candidate-model.vw</code>. In this instance we have two arms, so the command will look something like:</p>

<p><code class="highlighter-rouge">vw --cb 2 --cb_type ips -d train.dat -f candidate-model.vw</code></p>

<p>Feel free to add other options, including training over multiple passes with <code class="highlighter-rouge">--passes</code>. If you run the command as is above, you get the following result (actual results may vary based on VW version and the seed):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>§ vw --cb 2 --cb_type ips -d train.dat -f candidate-model.vw
final_regressor = candidate-model.vw
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train.dat
num sources = 1
Enabled reductions: gd, scorer, csoaa, cb
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
2.000000 2.000000            1            1.0    known        1        2
1.000000 0.000000            2            2.0    known        2        2
1.000000 1.000000            4            4.0    known        1        2
0.750000 0.500000            8            8.0    known        1        2
0.500000 0.250000           16           16.0    known        1        2

finished run
number of examples = 16
weighted example sum = 16.000000
weighted label sum = 0.000000
average loss = 0.500000
total feature number = 32
</code></pre></div></div>

<p>The <code class="highlighter-rouge">average loss</code> above is of less interest to us in this scenario since we have a separate test set. Let’s load <code class="highlighter-rouge">candidate-model.vw</code> using <code class="highlighter-rouge">-i</code> and test against our test set. Remember to use the <code class="highlighter-rouge">-t</code> flag to disable learning, and to use the same <code class="highlighter-rouge">cb_type</code> as you did when training:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vw -i candidate-model.vw --cb_type ips -t -d test.dat
only testing
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = test.dat
num sources = 1
Enabled reductions: gd, scorer, csoaa, cb
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0    known        1        2
0.000000 0.000000            2            2.0    known        1        2
0.500000 1.000000            4            4.0    known        1        2
0.250000 0.000000            8            8.0    known        1        2

finished run
number of examples = 8
weighted example sum = 8.000000
weighted label sum = 0.000000
average loss = 0.250000
total feature number = 16
</code></pre></div></div>

<p>The <code class="highlighter-rouge">average loss</code> reported is the OPE estimate for this policy, and in this case, calculated against our test set. Since we specified <code class="highlighter-rouge">--cb_type ips</code>, the IPS estimator is used, which is unbiased. Feel free to use <code class="highlighter-rouge">dr</code>, too, but note that although VW will allow it, <em>the use of <code class="highlighter-rouge">dm</code> is discouraged for OPE since it is biased</em>. If you are unsure, we suggest using <code class="highlighter-rouge">dr</code>. Note that VW will complain if you train using one <code class="highlighter-rouge">cb_type</code> but test using another; mixing estimators in training and evaluation is currently not supported.</p>

<p>Now that you have an OPE estimate of <code class="highlighter-rouge">0.250000</code>, how does it compare to the production policy in production? This comparison is easy to make since, for the same time period, we have the ground truth in the <code class="highlighter-rouge">test.dat</code> file. If we sum the costs in that file and divide by the number of examples, we get <code class="highlighter-rouge">0.625</code>. Generally, our toy example has far too little data with which to perform reliable estimates, but the principle applied: lower is better, so your candidate policy is estimated to perform better than the production policy in production. The exact definition of OPE is important: in this case, it means that had you deployed the candidate policy, with no exploration, instead of the production policy you could have expected to see the average cost reduce from <code class="highlighter-rouge">0.625</code> to <code class="highlighter-rouge">0.250000</code> <em>for the period of time covered in the test set</em>.</p>

<p>Feel free to gridsearch several candidate policies using the same setup, to determine a combination of hyperparameters that work well for your use case. Note however that mixing different <code class="highlighter-rouge">cb_type</code> options when gridsearching is discouraged, even if all specified estimators are unbiased. Choose either IPS or DR beforehand.</p>

<h2 id="batch-scenario-policy-evaluation-with-a-pre-trained-vw-policy-cb_adf-format-data">Batch scenario: policy evaluation with a pre-trained VW policy, <code class="highlighter-rouge">cb_adf</code>-format data</h2>

<p>The <code class="highlighter-rouge">cb_adf</code> format is especially useful if you have rich features associated with an arm, or a variable number of arms per round. If you have <code class="highlighter-rouge">adf</code>-format data, the same procedure as above applies – just change <code class="highlighter-rouge">cb</code> to <code class="highlighter-rouge">cb_adf</code> in the corresponding commands.</p>

<h2 id="online-scenario-policy-evaluation-with-an-incrementally-trained-vw-policy-cb-format-data">Online scenario: policy evaluation with an incrementally trained VW policy, <code class="highlighter-rouge">cb</code>-format data</h2>

<p>In the online scenario, when you deploy a new policy behind e.g. a REST endpoint, that policy will continue to update itself from second to second, as and when new examples come in. Learning is incremental: you don’t iterate over the same training examples more than once.</p>

<p>Online learning is particularly useful in settings where you need to react to changes in the world as fast as possible. From the point of view of OPE, the objective is the same: determining if a candidate policy is better than the one currently in production. But the setup differs slightly: since any policy deployed will continue to learn online, the key question is how well it will generalise to new examples coming in, <em>considering</em> that the policy is constantly evolving. So in this case, our candidate policy is in fact not a fixed policy, but one that changes constantly. It may help to think of it as a set of hyperparameters instead. The aim is to find out which set of these hyperparameters learns best in an online fashion.</p>

<p>To answer this question, we leverage <em>progressive validation</em> (PV), a validation process implemented in VW. PV is explained in detail elsewhere (see e.g. <a href="https://vimeo.com/240429210">John Langford’s talk on real world interactive learning</a>), but for this tutorial, it is enough to know that for one-pass learning, the loss reported by progressive validation deviates like a test set yet allows you to train on all of your data. It’s a good indicator of the generalisation performance for online learning.</p>

<p>VW reports PV loss automatically if you only iterate once over your data when training, i.e. <code class="highlighter-rouge">--passes</code> is <code class="highlighter-rouge">1</code> (the default). In this case, obtaining an OPE estimate is simply a matter of taking the bandit data from the production policy, again ordered oldest example first, and training as normal – no train/test split is needed.</p>

<p>First, save your bandit data to a file, e.g <code class="highlighter-rouge">data.dat</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1:1:0.5 | user_age:25
2:0:0.5 | user_age:25
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
1:1:0.5 | user_age:27
2:0:0.5 | user_age:21
2:0:0.5 | user_age:23
2:0:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:36
1:1:0.5 | user_age:25
2:0:0.5 | user_age:25
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:56
1:1:0.5 | user_age:27
2:0:0.5 | user_age:21
2:0:0.5 | user_age:23
2:0:0.5 | user_age:56
2:1:0.5 | user_age:55
2:1:0.5 | user_age:36
</code></pre></div></div>

<p>Then, train a policy incrementally. For the above data, with 2 possible actions and an IPS estimator, the command would be <code class="highlighter-rouge">vw --cb 2 --cb_type ips -d data.dat</code> (plus any additional options).</p>

<p>The result should look something like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>§ vw --cb 2 --cb_type ips -d data.dat 
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = full.dat
num sources = 1
Enabled reductions: gd, scorer, csoaa, cb
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
2.000000 2.000000            1            1.0    known        1        2
1.000000 0.000000            2            2.0    known        2        2
1.000000 1.000000            4            4.0    known        1        2
0.750000 0.500000            8            8.0    known        1        2
0.500000 0.250000           16           16.0    known        1        2

finished run
number of examples = 24
weighted example sum = 24.000000
weighted label sum = 0.000000
average loss = 0.416667
total feature number = 48
</code></pre></div></div>

<p>The <code class="highlighter-rouge">average loss</code> here is the OPE estimate, calculated using progressive validation, with the <code class="highlighter-rouge">cb_type</code> estimator you specified. The same caveats as before apply: if the specified <code class="highlighter-rouge">cb_type</code> is biased, it is generally no recommended to use the average loss as an OPE estimate. Again, if you are unsure, we recommend using the default by omitting <code class="highlighter-rouge">cb_type</code> altogether.</p>

<p>To compare the OPE estimate of the candidate policy to the production policy, calculate the realised average cost in the <code class="highlighter-rouge">data.dat</code> file. In this case, it is <code class="highlighter-rouge">0.6667</code>, and since our candidate policy’s <code class="highlighter-rouge">0.416667</code> is lower, we have found a set of hyperparameters estimated to perform better than our production online learner were it deployed at the time covered by the data.</p>

<h2 id="online-scenario-policy-evaluation-with-an-incrementally-trained-vw-policy-cb_adf-format-data">Online scenario: policy evaluation with an incrementally trained VW policy, <code class="highlighter-rouge">cb_adf</code>-format data</h2>

<p>If you have <code class="highlighter-rouge">adf</code>-format data, the same procedure as above applies – just change <code class="highlighter-rouge">cb</code> to <code class="highlighter-rouge">cb_adf</code> in the corresponding commands.</p>

<h2 id="legacy-policy-evaluation-with-cb-format-data-using-a-pre-trained-policy">Legacy: policy evaluation with <code class="highlighter-rouge">cb</code>-format data, using a pre-trained policy</h2>

<p>If your production policy produces bandit data in the standard <code class="highlighter-rouge">cb</code> format, and you already have a candidate policy even one trained outside VW, you can use the legacy <code class="highlighter-rouge">--eval</code> option to perform OPE. It is not recommended to use <code class="highlighter-rouge">--eval</code> if you are able to use any of the other methods described in this tutorial.</p>

<p>First, create a new file, e.g. <code class="highlighter-rouge">eval.dat</code>. Then, for each instance of your production policy’s bandit data, write it to <code class="highlighter-rouge">eval.dat</code> but prepend the line with the action your candidate policy would have chosen given the same context. For example, if your current instance is <code class="highlighter-rouge">1:2:0.5 | feature_a feature_b</code> and your candidate policy chooses action 2 instead given the same context <code class="highlighter-rouge">feature_a feature_b</code>, write the line <code class="highlighter-rouge">2 1:2:0.5 | feature_a feature_b</code> (note the space!).</p>

<p>After you’ve written your data file, it might look something like this:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2 1:2:0.5 | feature_a feature_b
2 2:2:0.4 | feature_a feature_c
1 1:2:0.1 | feature_b feature_c
</code></pre></div></div>

<p>In the toy example above, the candidate agreed with the production policy for the second and third instances, but disagreed on the first instance.</p>

<p>You are now ready to run policy evaluation using the command <code class="highlighter-rouge">vw --cb &lt;number_of_arms&gt; --eval -d &lt;dataset&gt;</code>. In our example, we have two possible actions, so the command is <code class="highlighter-rouge">vw --cb 2 --eval -d eval.dat</code>. This produced the following output (your results might differ based on VW version, or the seed):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = eval.dat
num sources = 1
Enabled reductions: gd, scorer, csoaa, cb
average since     example    example current current current
loss   last     counter     weight  label predict features
0.000000 0.000000      1      1.0  known    2    3
2.500000 5.000000      2      2.0  known    2    3

finished run
number of examples = 3
weighted example sum = 3.000000
weighted label sum = 0.000000
average loss = 6.501957
total feature number = 9
</code></pre></div></div>

<p>Again, <code class="highlighter-rouge">average loss</code> is the OPE estimate, and can be compared against the production policy’s realised average loss to determine if your candidate policy is estimated to work better than the policy in production.</p>

<p>Note what happens if we try to run <code class="highlighter-rouge">--eval</code> with an estimator we know is biased, <code class="highlighter-rouge">vw --cb 2 --eval -d eval.dat --cb_type dm</code>. You will end up with an error, to prevent you from making a mistake:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error: direct method can not be used for evaluation --- it is biased.

finished run
number of examples = 0
weighted example sum = 0.000000
weighted label sum = 0.000000
average loss = n.a.
total feature number = 0
direct method can not be used for evaluation --- it is biased.
vw (cb_algs.cc:161): direct method can not be used for evaluation --- it is biased.
</code></pre></div></div>

<h2 id="more-to-explore">More to explore</h2>

<ul>
  <li>Explore more Vowpal Wabbit <a href="../tutorials.html">Tutorials</a>.</li>
  <li>Browse <a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Examples" target="blank">examples on the GitHub wiki</a>.</li>
</ul>

        </div>
      </div>
    </div>

    <div class="container bibliography_container">
  <div class="header">
    <h3>
      CITATIONS
    </h3>
    <div class="all_publications_link">
      <a href="/research.html">
        All publications
      </a>
    </div>
  </div>

  <ol class="bibliography"></ol>
</div>

    <div class="container-fluid footer_container">
  <div class="container">
    <div class="row align-items-center justify-content-between">
      <div class="col">
        Microsoft is a major contributor
      </div>
      <div class="col feedback_col">
        <a
          class="feedback_link"
          href="https://github.com/VowpalWabbit/vowpalwabbit.github.io/issues/new"
          target="_blank"
        >
          Feedback
        </a>
      </div>
    </div>
  </div>
</div>

<script src="/assets/js/citation.js"></script>

    <div class="overlay">
    </div>

    <script src="/assets/js/tutorial.js"></script>
  </body>
</html>
